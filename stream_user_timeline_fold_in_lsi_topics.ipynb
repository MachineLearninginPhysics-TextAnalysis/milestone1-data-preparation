{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stream_user_timeline_fold_in_lsi_topics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdkp8Zkr4N3O"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ISY1HBmxu_R"
      },
      "source": [
        "import tweepy as tw\n",
        "\n",
        "# define keys\n",
        "consumer_key= 'gEJhQtgiIvxzNB50u4JPic8f4'\n",
        "consumer_secret= 'iEfTG65lFX8cAzKJ4QIhJklvuh3tfWdaRAAWO3b17082dZaSiu'\n",
        "access_token= '1369691334051852293-IGWGrIUKFY6rTwmrA5WD3YLkJrlUk5'\n",
        "access_token_secret= '7ftmxYiYnso7PNkPOWOWKCkNrguFFMhwwPTHhQ6bFVvgG'\n",
        "# authenticate and create api object\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYE-cpK_6wU"
      },
      "source": [
        "import re\n",
        "\n",
        "# just see how iterate through first 3 user's timeline pages with cursor\n",
        "for page in tw.Cursor(api.user_timeline,\n",
        "                      id=\"indykaila\", exclude_replies=True,\n",
        "                      include_rts=False, tweet_mode='extended').pages(3):\n",
        "    for status in page:\n",
        "        # preprocess documents (remove links and punctuations) to raw texts\n",
        "        print(status.full_text)\n",
        "        print(status.full_text.lower().split())\n",
        "        link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "        punc_link_removed = re.sub(r'[–,-.!\":]\\D', ' ', link_removed)\n",
        "        print(punc_link_removed.split(), '\\n---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZRTny0gDR8a"
      },
      "source": [
        "# Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32uHaNm9TTHJ"
      },
      "source": [
        "class MyTexts:\n",
        "    \"\"\"implement a generator object on a specific user timeline\n",
        "    and preprocess (remove links and punctuations)\n",
        "    to yield as row tokenized texts\"\"\"\n",
        "    def __init__(self, pagination_num=3):\n",
        "        self.pagination_num = pagination_num\n",
        "        # cursor on user's timeline\n",
        "        self.cursor = tw.Cursor(api.user_timeline, id=\"indykaila\",\n",
        "                              exclude_replies=True, include_rts=False,\n",
        "                              tweet_mode='extended').pages(self.pagination_num)\n",
        "    def __iter__(self):\n",
        "        for page in self.cursor:\n",
        "            for status in page:\n",
        "                # cleaning: removing links and some punctuations\n",
        "                link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "                punc_link_removed = re.sub(r'[–,-.!\":]\\D', '', link_removed)\n",
        "                yield punc_link_removed.split()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVWYqLhpDgR9"
      },
      "source": [
        "> Collect statistics about corpus, preprocess it and store training corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1adkuGWTlac",
        "outputId": "70cc1e78-35a3-4028-d0bf-7fab9f0950b5"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "texts = MyTexts(3)\n",
        "# colloct statistics about all tokens\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# preprocess: remove stop words and only once words from dictionary\n",
        "stop_words = set('for of a an and the to in'.split())\n",
        "stop_word_ids = [dictionary.token2id[stopword]\n",
        "                 for stopword in stop_words\n",
        "                 if stopword in dictionary.token2id]\n",
        "once_word_ids = [tokenid\n",
        "                 for tokenid, docfreq in dictionary.dfs.items()\n",
        "                 if docfreq == 1]\n",
        "dictionary.filter_tokens(stop_word_ids + once_word_ids)\n",
        "dictionary.compactify()\n",
        "print(dictionary.dfs)\n",
        "# store training corpus in bow representation for later use\n",
        "# prefer work with generators (corpus streaming) instead of creating lists of documents\n",
        "# train_texts = [text for text in MyTexts(3)]\n",
        "train_texts = MyTexts(3)\n",
        "'''note that every token that removed from dictionary is a \"blah\" for doc2bow method\n",
        "and wouldn't count in corpus bow representation\n",
        "but I use them in train_texts just for see the full texts somewhere'''\n",
        "# train_corpus_bow = [dictionary.doc2bow(text) for text in train_texts]\n",
        "train_corpus_bow = (dictionary.doc2bow(text) for text in train_texts)\n",
        "corpora.MmCorpus.serialize('train_corpus_bow.mm', train_corpus_bow)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-01 13:32:06,546 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2021-04-01 13:32:06,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-04-01 13:32:07,147 : INFO : built Dictionary(330 unique tokens: ['a', 'be', 'due', 'for', 'injury#lfc']...) from 29 documents (total 547 corpus positions)\n",
            "2021-04-01 13:32:07,153 : INFO : storing corpus in Matrix Market format to train_corpus_bow.mm\n",
            "2021-04-01 13:32:07,159 : INFO : saving sparse matrix to train_corpus_bow.mm\n",
            "2021-04-01 13:32:07,333 : INFO : PROGRESS: saving document #0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{3: 3, 4: 3, 0: 4, 2: 3, 1: 2, 6: 5, 5: 2, 7: 6, 8: 7, 10: 2, 15: 6, 12: 2, 13: 5, 16: 4, 19: 2, 20: 2, 18: 2, 9: 2, 11: 3, 17: 3, 21: 2, 14: 2, 23: 3, 25: 2, 24: 2, 22: 2, 29: 5, 26: 3, 28: 2, 27: 2, 31: 2, 30: 2, 32: 2, 35: 2, 33: 4, 36: 2, 37: 2, 34: 2, 38: 2, 39: 3, 40: 2, 41: 2, 42: 6, 44: 3, 45: 2, 43: 2, 48: 2, 47: 2, 46: 2, 50: 2, 51: 3, 49: 6, 52: 2, 54: 2, 53: 2, 57: 2, 55: 2, 59: 3, 56: 2, 58: 2, 60: 2, 62: 2, 63: 3, 61: 2, 64: 2, 65: 2, 66: 3}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-01 13:32:07,694 : INFO : saved 29x67 matrix, density=9.418% (183/1943)\n",
            "2021-04-01 13:32:07,698 : INFO : saving MmCorpus index to train_corpus_bow.mm.index\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGZ4AT-4aHMu"
      },
      "source": [
        "dictionary.dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MT2DVuCXvjy"
      },
      "source": [
        "train_texts = MyTexts(3)\n",
        "for text in train_texts:\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu0FZthg1uDE"
      },
      "source": [
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')\n",
        "for doc in train_corpus_bow:\n",
        "    print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLN3MOSAC43C"
      },
      "source": [
        "# Train the Models with Training Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC1O4vBB4jsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c384b8-4f18-42f1-ffb3-86a210f244e4"
      },
      "source": [
        "from gensim import models\n",
        "\n",
        "# load training corpus in bow from disk\n",
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')\n",
        "# initialize a tfidf model with training corpus in bow: training\n",
        "tfidf_model = models.TfidfModel(train_corpus_bow)\n",
        "# tranform training corpus bow->tfidf\n",
        "train_corpus_tfidf = tfidf_model[train_corpus_bow]\n",
        "# train a LSI model with training corpus in tfidf\n",
        "lsi_model = models.LsiModel(train_corpus_tfidf, id2word=dictionary, num_topics=2)\n",
        "# tfidf->fold-in-lsi\n",
        "train_corpus_lsi = lsi_model[train_corpus_tfidf]\n",
        "train_corpus_lsi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-01 13:32:29,902 : INFO : loaded corpus index from train_corpus_bow.mm.index\n",
            "2021-04-01 13:32:29,908 : INFO : initializing cython corpus reader from train_corpus_bow.mm\n",
            "2021-04-01 13:32:29,914 : INFO : accepted corpus with 29 documents, 67 features, 183 non-zero entries\n",
            "2021-04-01 13:32:29,918 : INFO : collecting document frequencies\n",
            "2021-04-01 13:32:29,922 : INFO : PROGRESS: processing document #0\n",
            "2021-04-01 13:32:29,924 : INFO : calculating IDF weights for 29 documents and 66 features (183 matrix non-zeros)\n",
            "2021-04-01 13:32:29,926 : INFO : using serial LSI version on this node\n",
            "2021-04-01 13:32:29,928 : INFO : updating model with new documents\n",
            "2021-04-01 13:32:29,932 : INFO : preparing a new chunk of documents\n",
            "2021-04-01 13:32:29,939 : INFO : using 100 extra samples and 2 power iterations\n",
            "2021-04-01 13:32:29,941 : INFO : 1st phase: constructing (67, 102) action matrix\n",
            "2021-04-01 13:32:29,948 : INFO : orthonormalizing (67, 102) action matrix\n",
            "2021-04-01 13:32:29,969 : INFO : 2nd phase: running dense svd on (67, 29) matrix\n",
            "2021-04-01 13:32:29,978 : INFO : computing the final decomposition\n",
            "2021-04-01 13:32:29,982 : INFO : keeping 2 factors (discarding 80.669% of energy spectrum)\n",
            "2021-04-01 13:32:29,986 : INFO : processed documents up to #29\n",
            "2021-04-01 13:32:29,990 : INFO : topic #0(1.690): 0.428*\"is\" + 0.330*\"#nufc\" + 0.200*\"with\" + 0.188*\"at\" + 0.185*\"have\" + 0.168*\"out\" + 0.163*\"has\" + 0.150*\"no\" + 0.149*\"will\" + 0.146*\"want\"\n",
            "2021-04-01 13:32:29,991 : INFO : topic #1(1.537): 0.646*\"#nufc\" + 0.425*\"is\" + -0.225*\"news\" + -0.182*\"all\" + -0.135*\"at\" + -0.129*\"with\" + -0.121*\"chance\" + -0.112*\"manchester\" + 0.110*\"want\" + -0.105*\"no\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.interfaces.TransformedCorpus at 0x7f6396112dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C5t7o3LBGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd1c616-f9aa-4f74-b15f-6375ccde5066"
      },
      "source": [
        "lsi_model.print_topics()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-01 13:32:34,501 : INFO : topic #0(1.690): 0.428*\"is\" + 0.330*\"#nufc\" + 0.200*\"with\" + 0.188*\"at\" + 0.185*\"have\" + 0.168*\"out\" + 0.163*\"has\" + 0.150*\"no\" + 0.149*\"will\" + 0.146*\"want\"\n",
            "2021-04-01 13:32:34,502 : INFO : topic #1(1.537): 0.646*\"#nufc\" + 0.425*\"is\" + -0.225*\"news\" + -0.182*\"all\" + -0.135*\"at\" + -0.129*\"with\" + -0.121*\"chance\" + -0.112*\"manchester\" + 0.110*\"want\" + -0.105*\"no\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.428*\"is\" + 0.330*\"#nufc\" + 0.200*\"with\" + 0.188*\"at\" + 0.185*\"have\" + 0.168*\"out\" + 0.163*\"has\" + 0.150*\"no\" + 0.149*\"will\" + 0.146*\"want\"'),\n",
              " (1,\n",
              "  '0.646*\"#nufc\" + 0.425*\"is\" + -0.225*\"news\" + -0.182*\"all\" + -0.135*\"at\" + -0.129*\"with\" + -0.121*\"chance\" + -0.112*\"manchester\" + 0.110*\"want\" + -0.105*\"no\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e59_on5K-1eJ",
        "outputId": "73512960-3608-4e40-dc6a-02166b5382be"
      },
      "source": [
        "train_texts = MyTexts(3)\n",
        "for doc_lsi, as_text in zip(train_corpus_lsi, train_texts):\n",
        "    print(doc_lsi, as_text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.2634753900441857), (1, -0.06353021807784559)] ['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury#lfc']\n",
            "[(0, 0.3787829963532377), (1, -0.14531977919418487)] ['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.']\n",
            "[(0, 0.34944563689622904), (1, -0.250849484693159)] ['sergio', 'aguero', 'has', 'been', 'advised', 'by', 'his', 'public', 'relations', 'team', 'not', 'to', 'move', 'to', '@chelseafc', '@fcbarcelona', 'are', 'leading', 'the', 'racetalks', 'to', 'start', 'next', 'week', 'with', 'the', 'spanish', 'club.']\n",
            "[(0, 0.24920297656699164), (1, -0.01482318304162268)] ['if', 'you', 'want', 'to', 'make', 'some', 'extra', 'money', 'tomorrow', 'give', '@betciub', 'a', 'followthey', 'have', 'some', 'huge', 'winners', '💷', '#ad']\n",
            "[(0, 0.25317608579162876), (1, -0.1780255774122605)] ['manchester', 'city', 'confirm', 'sergio', 'aguero', 'will', 'leave', 'club', 'this', 'summer']\n",
            "[(0, 0.19321519904889728), (1, -0.2704503966397409)] ['everyone', 'connected', 'to', '@ecomi_', 'deserve', 'all', 'the', 'praise', 'they', 'can', 'getmind', 'blowing', 'licenses', 'in', 'the', 'pipelineone', 'particular', 'license', 'will', 'excite', 'all', 'nft', 'fans', 'around', 'the', 'worldcongratulations', 'to', 'all', 'long', 'term', 'holders', 'in', '#omi', '💪']\n",
            "[(0, 0.17355336170131563), (1, -0.16744054694727634)] ['ibrahima', 'konate', 'on', 'links', 'to', '@lfc', 'woke', 'upi', 'saw', 'my', 'phone', 'ringing', 'all', 'over', 'the', 'placei', 'just', 'wanted', 'to', 'tell', \"them'heyi\", 'played', \"yesterdayi'm\", 'tiredleave', 'me', 'alone#lfc']\n",
            "[(0, 0.4284953214968911), (1, 0.4248704179119842)] ['roberto', 'firmino', 'is', 'back', 'in', 'full', 'training#lfc']\n",
            "[(0, 0.3197363737788343), (1, 0.11601395967234425)] ['breakingliverpool', 'looking', 'to', 'sign', 'ibrahima', 'konatethe', 'deal', 'is', 'close', 'according', 'to', 'our', 'friend', '@david_ornstein', '#lfc']\n",
            "[(0, 0.3326093175138377), (1, -0.23932095983065496)] ['erling', 'haaland', 'has', 'held', 'talks', 'with', '@chelseafcconfirmedthey', 'are', 'trying', 'to', 'break', 'up', 'his', 'agreement', 'with', '@mancity400k', 'a', 'week', 'offered', 'by', '#cfc']\n",
            "[(0, 0.2818713331337689), (1, -0.37265265777340606)] ['we', 'breaking', '@chelseafc', 'news', 'at', '7pmthe', 'news', 'will', 'excite', '#cfc', 'fans', 'all', 'over', 'the', 'worldbig', 'news#chelsea']\n",
            "[] ['perfect', 'for', '#liverpool', 'fans£4.99', 'special', 'offer']\n",
            "[(0, 0.188376028528787), (1, -0.2904354091632472)] ['hearing', 'big', 'news', 'for', '@ecomi_', 'next', 'weekthey', 'deserve', 'all', 'the', 'luckgame', 'changer#omi', '#veve']\n",
            "[(0, 0.3380140979402746), (1, -0.18838459182197786)] ['france', 'have', 'confirmed', 'that', 'n’golo', 'kante', 'has', 'suffered', 'a', 'small', 'left', 'hamstring', 'tear', 'with', 'the', 'midfielder', 'set', 'to', 'return', 'to', 'chelsea', 'for', 'rehabilitation#cfc']\n",
            "[(0, 0.384015849590737), (1, -0.1430535578244324)] ['ole', 'gunnar', 'solskjaer', 'will', 'be', 'handed', 'a', 'new', '£30million', 'deal', 'by', 'manchester', 'united', 'even', 'if', 'he', 'fails', 'to', 'win', 'a', 'trophy', 'this', 'season', 'ed', 'woodward', 'you’re', 'a', '🤐']\n",
            "[] ['martin', 'ødegaard', 'injured', 'for', 'norwayankle', 'injury#afc']\n",
            "[(0, 0.30388567660384513), (1, -0.23991811020523174)] ['so', 'if', 'brendan', 'rodgers', 'wanted', 'to', 'try', 'his', 'luck', 'again', 'with', 'so', 'called', 'big', 'english', 'clubs', 'in', 'the', 'summer', 'liverpool', 'no', 'chance', 'manchester', 'united', 'due', 'to', 'liverpool', 'linksno', 'chance', 'chelsea', 'no', 'chance', 'manchester', 'city', 'no', 'chance', 'arsenal', 'no', 'chance', 'yet', 'spurs', 'yescan', 'happen']\n",
            "[(0, 0.5700630150537481), (1, -0.1115186063178031)] ['harry', 'kane', 'has', 'told', '@spursofficial', 'not', 'to', 'increase', 'his', 'transfer', 'value', 'to', 'high', 'levelshe', 'wants', 'to', 'leave', '#thfc', 'for', 'around', '£90m', '£100mhe', 'is', 'worried', 'that', 'no', 'clubs', 'will', 'pay', 'the', 'asking', 'value', 'set', 'by', '#spurs', 'at', '£130m', 'he', 'is', 'willing', 'to', 'go', 'public', 'with', 'a', 'transfer', 'request', 'at', 'last', 'resort']\n",
            "[(0, 0.3848169217377475), (1, -0.1290459981836985)] ['massimiliano', 'allegri', 'has', 'met', 'the', 'glazer', 'family', 'in', 'americathe', 'meeting', 'lasted', '3', 'hoursed', 'woodward', 'is', 'trying', 'keep', 'ole', 'gunnar', 'solskjær', 'at', 'the', 'club', 'as', 'manager', 'for', 'next', 'seasoncertain', '#mufc', 'board', 'members', 'are', 'unsure', 'on', 'the', 'progress', 'been', 'made', 'by', 'ole', 'gunnar', 'solskjær']\n",
            "[(0, 0.24794187910281382), (1, -0.3037524039102338)] ['major', 'manchester', 'united', 'news', 'breaking', 'at', '10pmit’s', 'bad', 'news', 'for', 'the', 'manager#mufc', '@manutd']\n",
            "[(0, 0.4134878027179103), (1, -0.22930186740719122)] ['manchester', 'city', 'have', 'agreement', 'in', 'place', 'with', '@erlinghaalandthe', 'deal', 'was', 'agreed', 'last', 'weekthe', 'information', 'has', 'come', 'from', 'our', 'source', 'at', '@mancity', '#mcfc.']\n",
            "[(0, 0.3476334031418761), (1, 0.45108807039761234)] ['out', 'for', 'the', 'season', '#nufc']\n",
            "[(0, 0.329507215415028), (1, 0.6457493744616041)] ['isaac', 'hayden', '😔', '#nufc']\n",
            "[(0, 0.10694498755152422), (1, 0.02608373414187217)] ['and', 'you', 'doubted', 'usrespected', 'reporter', '@david_ornstein', 'also', 'reporting', 'the', 'same']\n",
            "[(0, 0.15836342836887757), (1, -0.10310712988724095)] ['serious', 'questions', 'need', 'to', 'be', 'asked', 'of', 'aubameyang', 'as', '1st', 'team', 'player', '&amp;', 'captain', 'of', 'the', 'club.']\n",
            "[(0, 0.14998043796493143), (1, -0.10540228448948802)] ['no', 'option', 'to', 'buy', 'in', 'the', 'odegaard', '#afc', 'loan', 'deal.']\n",
            "[(0, 0.5150622739896622), (1, 0.7727430923231855)] ['mike', 'ashley', 'is', 'in', 'the', 'middle', 'east👀', '#nufc']\n",
            "[(0, 0.3436995569539927), (1, 0.3106300195448718)] ['i', 'want', 'steve', 'bruce', 'sacked', 'tonightenough', 'is', 'enough', 'for', 'god', 'sake#nufc.']\n",
            "[(0, 0.2923257654875742), (1, -0.20667705563079863)] ['celtic', 'have', 'decided', 'to', 'snub', 'the', 'chance', 'to', 'give', 'rangers', 'a', 'guard', 'of', 'honour', 'when', 'the', 'two', 'sides', 'meet', 'in', \"sunday's\", 'old', 'firm', 'derbythe', 'two', 'sides', 'clash', 'at', 'celtic', 'park', 'on', 'sunday']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}