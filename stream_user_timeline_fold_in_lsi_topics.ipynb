{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stream_user_timeline_fold_in_lsi_topics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdkp8Zkr4N3O"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ISY1HBmxu_R"
      },
      "source": [
        "import tweepy as tw\n",
        "\n",
        "# define keys\n",
        "consumer_key= 'gEJhQtgiIvxzNB50u4JPic8f4'\n",
        "consumer_secret= 'iEfTG65lFX8cAzKJ4QIhJklvuh3tfWdaRAAWO3b17082dZaSiu'\n",
        "access_token= '1369691334051852293-IGWGrIUKFY6rTwmrA5WD3YLkJrlUk5'\n",
        "access_token_secret= '7ftmxYiYnso7PNkPOWOWKCkNrguFFMhwwPTHhQ6bFVvgG'\n",
        "# authenticate and create api object\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYE-cpK_6wU"
      },
      "source": [
        "import re\n",
        "\n",
        "# just see how iterate through first 3 user's timeline pages with cursor\n",
        "for page in tw.Cursor(api.user_timeline,\n",
        "                      id=\"indykaila\", exclude_replies=True,\n",
        "                      include_rts=False, tweet_mode='extended').pages(3):\n",
        "    for status in page:\n",
        "        # preprocess documents (remove links and punctuations) to raw texts\n",
        "        print(status.full_text)\n",
        "        print(status.full_text.lower().split())\n",
        "        link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "        punc_link_removed = re.sub(r'[–,-.!\":]\\D', ' ', link_removed)\n",
        "        print(punc_link_removed.split(), '\\n---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZRTny0gDR8a"
      },
      "source": [
        "# Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32uHaNm9TTHJ"
      },
      "source": [
        "class MyTexts:\n",
        "    \"\"\"implement a generator object on a specific user timeline\n",
        "    and preprocess (remove links and punctuations)\n",
        "    to yield as row tokenized texts\"\"\"\n",
        "    def __init__(self, pagination_num=3):\n",
        "        self.pagination_num = pagination_num\n",
        "        # cursor on user's timeline\n",
        "        self.cursor = tw.Cursor(api.user_timeline, id=\"indykaila\",\n",
        "                              exclude_replies=True, include_rts=False,\n",
        "                              tweet_mode='extended').pages(self.pagination_num)\n",
        "    def __iter__(self):\n",
        "        for page in self.cursor:\n",
        "            for status in page:\n",
        "                # cleaning: removing links and some punctuations\n",
        "                link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "                punc_link_removed = re.sub(r'[–,-.!\":]\\D', '', link_removed)\n",
        "                yield punc_link_removed.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVWYqLhpDgR9"
      },
      "source": [
        "> Collect statistics about corpus, preprocess it and store training corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1adkuGWTlac",
        "outputId": "45df57a7-56c1-4959-9548-2cea0f9b3de1"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "texts = MyTexts(3)\n",
        "# colloct statistics about all tokens\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# preprocess: remove stop words and only once words from dictionary\n",
        "stop_words = set('for of a an and the to in'.split())\n",
        "stop_word_ids = [dictionary.token2id[stopword]\n",
        "                 for stopword in stop_words\n",
        "                 if stopword in dictionary.token2id]\n",
        "once_word_ids = [tokenid\n",
        "                 for tokenid, docfreq in dictionary.dfs.items()\n",
        "                 if docfreq == 1]\n",
        "dictionary.filter_tokens(stop_word_ids + once_word_ids)\n",
        "dictionary.compactify()\n",
        "print(dictionary.dfs)\n",
        "# store training corpus in bow representation for later use\n",
        "# prefer work with generators (corpus streaming) instead of creating lists of documents\n",
        "# train_texts = [text for text in MyTexts(3)]\n",
        "train_texts = MyTexts(3)\n",
        "'''note that every token that removed from dictionary is a \"blah\" for doc2bow method\n",
        "and wouldn't count in corpus bow representation\n",
        "but I use them in train_texts just for see the full texts somewhere'''\n",
        "# train_corpus_bow = [dictionary.doc2bow(text) for text in train_texts]\n",
        "train_corpus_bow = (dictionary.doc2bow(text) for text in train_texts)\n",
        "corpora.MmCorpus.serialize('train_corpus_bow.mm', train_corpus_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 10:29:33,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-03-31 10:29:33,886 : INFO : built Dictionary(327 unique tokens: ['4', 'a', 'bayern', 'be', 'confirmed']...) from 28 documents (total 535 corpus positions)\n",
            "2021-03-31 10:29:33,892 : INFO : storing corpus in Matrix Market format to train_corpus_bow.mm\n",
            "2021-03-31 10:29:33,894 : INFO : saving sparse matrix to train_corpus_bow.mm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{2: 5, 1: 2, 4: 6, 0: 3, 3: 2, 5: 7, 17: 2, 7: 2, 12: 6, 9: 2, 10: 5, 13: 4, 16: 2, 18: 2, 15: 2, 6: 2, 8: 3, 14: 3, 19: 2, 11: 2, 21: 3, 23: 2, 22: 2, 20: 2, 27: 5, 24: 3, 26: 2, 25: 2, 29: 2, 28: 2, 30: 2, 33: 2, 31: 4, 34: 2, 35: 2, 32: 2, 36: 2, 37: 3, 38: 2, 39: 2, 40: 6, 42: 3, 43: 2, 41: 2, 46: 2, 45: 2, 44: 2, 48: 2, 49: 3, 47: 6, 50: 2, 53: 2, 52: 2, 51: 2, 56: 2, 54: 2, 58: 3, 55: 2, 57: 2, 59: 2, 61: 2, 62: 3, 60: 2, 63: 2, 64: 2, 65: 3}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 10:29:34,423 : INFO : PROGRESS: saving document #0\n",
            "2021-03-31 10:29:35,434 : INFO : saved 28x66 matrix, density=9.578% (177/1848)\n",
            "2021-03-31 10:29:35,436 : INFO : saving MmCorpus index to train_corpus_bow.mm.index\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGZ4AT-4aHMu"
      },
      "source": [
        "dictionary.dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MT2DVuCXvjy"
      },
      "source": [
        "train_texts = MyTexts(3)\n",
        "for text in train_texts:\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu0FZthg1uDE"
      },
      "source": [
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')\n",
        "for doc in train_corpus_bow:\n",
        "    print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLN3MOSAC43C"
      },
      "source": [
        "# Train the Models with Training Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC1O4vBB4jsx"
      },
      "source": [
        "from gensim import models\n",
        "\n",
        "# load training corpus in bow from disk\n",
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')\n",
        "# initialize a tfidf model with training corpus in bow: training\n",
        "tfidf_model = models.TfidfModel(train_corpus_bow)\n",
        "# tranform training corpus bow->tfidf\n",
        "train_corpus_tfidf = tfidf_model[train_corpus_bow]\n",
        "# train a LSI model with training corpus in tfidf\n",
        "lsi_model = models.LsiModel(train_corpus_tfidf, id2word=dictionary, num_topics=2)\n",
        "# tfidf->fold-in-lsi\n",
        "train_corpus_lsi = lsi_model[train_corpus_tfidf]\n",
        "train_corpus_lsi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C5t7o3LBGyM"
      },
      "source": [
        "lsi_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e59_on5K-1eJ",
        "outputId": "5477fe5d-01bb-43b8-aceb-b1baf4f27b2d"
      },
      "source": [
        "train_texts = MyTexts(3)\n",
        "for doc_lsi, as_text in zip(train_corpus_lsi, train_texts):\n",
        "    print(doc_lsi, as_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, -0.32233195687890615), (1, 0.09858006339441101)] ['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.']\n",
            "[(0, -0.3347516749221574), (1, 0.24999741456584593)] ['sergio', 'aguero', 'has', 'been', 'advised', 'by', 'his', 'public', 'relations', 'team', 'not', 'to', 'move', 'to', '@chelseafc', '@fcbarcelona', 'are', 'leading', 'the', 'racetalks', 'to', 'start', 'next', 'week', 'with', 'the', 'spanish', 'club.']\n",
            "[(0, -0.2576089869413192), (1, 0.011923011264475453)] ['if', 'you', 'want', 'to', 'make', 'some', 'extra', 'money', 'tomorrow', 'give', '@betciub', 'a', 'followthey', 'have', 'some', 'huge', 'winners', '💷', '#ad']\n",
            "[(0, -0.23638720624149792), (1, 0.17809356494061007)] ['manchester', 'city', 'confirm', 'sergio', 'aguero', 'will', 'leave', 'club', 'this', 'summer']\n",
            "[(0, -0.20411216861734688), (1, 0.293945284881936)] ['everyone', 'connected', 'to', '@ecomi_', 'deserve', 'all', 'the', 'praise', 'they', 'can', 'getmind', 'blowing', 'licenses', 'in', 'the', 'pipelineone', 'particular', 'license', 'will', 'excite', 'all', 'nft', 'fans', 'around', 'the', 'worldcongratulations', 'to', 'all', 'long', 'term', 'holders', 'in', '#omi', '💪']\n",
            "[(0, -0.18697076296438925), (1, 0.18466362172127557)] ['ibrahima', 'konate', 'on', 'links', 'to', '@lfc', 'woke', 'upi', 'saw', 'my', 'phone', 'ringing', 'all', 'over', 'the', 'placei', 'just', 'wanted', 'to', 'tell', \"them'heyi\", 'played', \"yesterdayi'm\", 'tiredleave', 'me', 'alone#lfc']\n",
            "[(0, -0.45510749025180974), (1, -0.41486682206562914)] ['roberto', 'firmino', 'is', 'back', 'in', 'full', 'training#lfc']\n",
            "[(0, -0.3392780811567825), (1, -0.10592045639952195)] ['breakingliverpool', 'looking', 'to', 'sign', 'ibrahima', 'konatethe', 'deal', 'is', 'close', 'according', 'to', 'our', 'friend', '@david_ornstein', '#lfc']\n",
            "[(0, -0.3299822612760303), (1, 0.23816939845154336)] ['erling', 'haaland', 'has', 'held', 'talks', 'with', '@chelseafcconfirmedthey', 'are', 'trying', 'to', 'break', 'up', 'his', 'agreement', 'with', '@mancity400k', 'a', 'week', 'offered', 'by', '#cfc']\n",
            "[(0, -0.295753619928158), (1, 0.40132989836032534)] ['we', 'breaking', '@chelseafc', 'news', 'at', '7pmthe', 'news', 'will', 'excite', '#cfc', 'fans', 'all', 'over', 'the', 'worldbig', 'news#chelsea']\n",
            "[] ['perfect', 'for', '#liverpool', 'fans£4.99', 'special', 'offer']\n",
            "[(0, -0.20177573416991165), (1, 0.31678640740132646)] ['hearing', 'big', 'news', 'for', '@ecomi_', 'next', 'weekthey', 'deserve', 'all', 'the', 'luckgame', 'changer#omi', '#veve']\n",
            "[(0, -0.304743326809042), (1, 0.16444999461224302)] ['france', 'have', 'confirmed', 'that', 'n’golo', 'kante', 'has', 'suffered', 'a', 'small', 'left', 'hamstring', 'tear', 'with', 'the', 'midfielder', 'set', 'to', 'return', 'to', 'chelsea', 'for', 'rehabilitation#cfc']\n",
            "[(0, -0.3756079957909999), (1, 0.15012399876448812)] ['ole', 'gunnar', 'solskjaer', 'will', 'be', 'handed', 'a', 'new', '£30million', 'deal', 'by', 'manchester', 'united', 'even', 'if', 'he', 'fails', 'to', 'win', 'a', 'trophy', 'this', 'season', 'ed', 'woodward', 'you’re', 'a', '🤐']\n",
            "[] ['martin', 'ødegaard', 'injured', 'for', 'norwayankle', 'injury#afc']\n",
            "[(0, -0.3034732191996566), (1, 0.24717172362883066)] ['so', 'if', 'brendan', 'rodgers', 'wanted', 'to', 'try', 'his', 'luck', 'again', 'with', 'so', 'called', 'big', 'english', 'clubs', 'in', 'the', 'summer', 'liverpool', 'no', 'chance', 'manchester', 'united', 'due', 'to', 'liverpool', 'linksno', 'chance', 'chelsea', 'no', 'chance', 'manchester', 'city', 'no', 'chance', 'arsenal', 'no', 'chance', 'yet', 'spurs', 'yescan', 'happen']\n",
            "[(0, -0.567906994731022), (1, 0.1166618980986488)] ['harry', 'kane', 'has', 'told', '@spursofficial', 'not', 'to', 'increase', 'his', 'transfer', 'value', 'to', 'high', 'levelshe', 'wants', 'to', 'leave', '#thfc', 'for', 'around', '£90m', '£100mhe', 'is', 'worried', 'that', 'no', 'clubs', 'will', 'pay', 'the', 'asking', 'value', 'set', 'by', '#spurs', 'at', '£130m', 'he', 'is', 'willing', 'to', 'go', 'public', 'with', 'a', 'transfer', 'request', 'at', 'last', 'resort']\n",
            "[(0, -0.39254219894512904), (1, 0.13803485065192883)] ['massimiliano', 'allegri', 'has', 'met', 'the', 'glazer', 'family', 'in', 'americathe', 'meeting', 'lasted', '3', 'hoursed', 'woodward', 'is', 'trying', 'keep', 'ole', 'gunnar', 'solskjær', 'at', 'the', 'club', 'as', 'manager', 'for', 'next', 'seasoncertain', '#mufc', 'board', 'members', 'are', 'unsure', 'on', 'the', 'progress', 'been', 'made', 'by', 'ole', 'gunnar', 'solskjær']\n",
            "[(0, -0.25988084643985976), (1, 0.3273852330418297)] ['major', 'manchester', 'united', 'news', 'breaking', 'at', '10pmit’s', 'bad', 'news', 'for', 'the', 'manager#mufc', '@manutd']\n",
            "[(0, -0.41461177419248774), (1, 0.2284296107445679)] ['manchester', 'city', 'have', 'agreement', 'in', 'place', 'with', '@erlinghaalandthe', 'deal', 'was', 'agreed', 'last', 'weekthe', 'information', 'has', 'come', 'from', 'our', 'source', 'at', '@mancity', '#mcfc.']\n",
            "[(0, -0.3180015753027594), (1, -0.4344178118891458)] ['out', 'for', 'the', 'season', '#nufc']\n",
            "[(0, -0.3359637515305482), (1, -0.6293612332218363)] ['isaac', 'hayden', '😔', '#nufc']\n",
            "[(0, -0.11609469007883425), (1, -0.024654259229085467)] ['and', 'you', 'doubted', 'usrespected', 'reporter', '@david_ornstein', 'also', 'reporting', 'the', 'same']\n",
            "[(0, -0.14243732591755132), (1, 0.09615596195827436)] ['serious', 'questions', 'need', 'to', 'be', 'asked', 'of', 'aubameyang', 'as', '1st', 'team', 'player', '&amp;', 'captain', 'of', 'the', 'club.']\n",
            "[(0, -0.15474728989730774), (1, 0.11135884770195005)] ['no', 'option', 'to', 'buy', 'in', 'the', 'odegaard', '#afc', 'loan', 'deal.']\n",
            "[(0, -0.5349512194208403), (1, -0.7536316059092685)] ['mike', 'ashley', 'is', 'in', 'the', 'middle', 'east👀', '#nufc']\n",
            "[(0, -0.36615664091991607), (1, -0.30380705422507065)] ['i', 'want', 'steve', 'bruce', 'sacked', 'tonightenough', 'is', 'enough', 'for', 'god', 'sake#nufc.']\n",
            "[(0, -0.2977999392933862), (1, 0.21007138587440444)] ['celtic', 'have', 'decided', 'to', 'snub', 'the', 'chance', 'to', 'give', 'rangers', 'a', 'guard', 'of', 'honour', 'when', 'the', 'two', 'sides', 'meet', 'in', \"sunday's\", 'old', 'firm', 'derbythe', 'two', 'sides', 'clash', 'at', 'celtic', 'park', 'on', 'sunday']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}