{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stream_user_timeline_fold_in_lsi_topics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "pythonjvsc74a57bd03664346d1f0019af0e5c969b3ecddce222b26e81abc0bb3df58369e247ba0050",
      "display_name": "Python 3.7.3  ('.venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "metadata": {
      "interpreter": {
        "hash": "3664346d1f0019af0e5c969b3ecddce222b26e81abc0bb3df58369e247ba0050"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdkp8Zkr4N3O"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "Fill in your keys to access Twitter API"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ISY1HBmxu_R"
      },
      "source": [
        "import tweepy as tw\n",
        "\n",
        "# define keys\n",
        "consumer_key= 'gEJhQtgiIvxzNB50u4JPic8f4'\n",
        "consumer_secret= 'iEfTG65lFX8cAzKJ4QIhJklvuh3tfWdaRAAWO3b17082dZaSiu'\n",
        "access_token= '1369691334051852293-IGWGrIUKFY6rTwmrA5WD3YLkJrlUk5'\n",
        "access_token_secret= '7ftmxYiYnso7PNkPOWOWKCkNrguFFMhwwPTHhQ6bFVvgG'\n",
        "# authenticate and create api object\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "First of all, just to see the texts data and getting to know *tokenization* and *cleaning data*, we give [`tweepy.API.user_timeline`](https://docs.tweepy.org/en/v3.5.0/api.html#API.user_timeline) method to the [`tweepy.Cursor`](https://docs.tweepy.org/en/v3.4.0/cursor_tutorial.html#cursor-tutorial) object for iterating through a specific user's timeline. In this case, one [twitter user](https://twitter.com/indykaila) who tweets about football news is considered, with excluding replies and retweets.\n",
        "\n",
        "A brief cleaning data is executed. This contains removing *links* and some *puctuations* from tweets (= `status.full_text`).\n",
        "\n",
        "Let's iterate through first page of user's timeline, print *tweet* & *not cleaned tokenized tweet* & *cleaned tokenized tweet* for each status:\n",
        "\n",
        "> **Note**: we look at all texts lowercased."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "import re\n",
        "\n",
        "# just see how iterate through first user's timeline pages with cursor\n",
        "for page in tw.Cursor(api.user_timeline,\n",
        "                      id=\"indykaila\", exclude_replies=True,\n",
        "                      include_rts=False, tweet_mode='extended').pages(1):\n",
        "    for status in page:\n",
        "        # cleaning documents (remove links and punctuations) to raw texts\n",
        "        print(status.full_text)\n",
        "        print(status.full_text.lower().split())\n",
        "        # replacing links with an empty character\n",
        "        link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "        # replacing punctuations with a whitespace\n",
        "        punc_link_removed = re.sub(r'[â€“,-.!\":]\\D', ' ', link_removed)\n",
        "        print(punc_link_removed.split(), '\\n---')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SSYE-cpK_6wU"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statement: Chelsea FC is disgusted with posts on social media this evening targeting West Bromwich Albion player Callum Robinson.Â #CFC\n['statement:', 'chelsea', 'fc', 'is', 'disgusted', 'with', 'posts', 'on', 'social', 'media', 'this', 'evening', 'targeting', 'west', 'bromwich', 'albion', 'player', 'callum', 'robinson.', '#cfc']\n['statement', 'chelsea', 'fc', 'is', 'disgusted', 'with', 'posts', 'on', 'social', 'media', 'this', 'evening', 'targeting', 'west', 'bromwich', 'albion', 'player', 'callum', 'robinson', '#cfc'] \n---\nDavid Luiz could need surgery #AFC\n['david', 'luiz', 'could', 'need', 'surgery', '#afc']\n['david', 'luiz', 'could', 'need', 'surgery', '#afc'] \n---\nConfirmed: Liverpool will meet with Erling Haaland's representatives on Friday #LFC\n['confirmed:', 'liverpool', 'will', 'meet', 'with', 'erling', \"haaland's\", 'representatives', 'on', 'friday', '#lfc']\n['confirmed', 'liverpool', 'will', 'meet', 'with', 'erling', \"haaland's\", 'representatives', 'on', 'friday', '#lfc'] \n---\nThe mind blowing licence to be announced by @ecomi_. Well played @vevecollectible. The company in great hands. #OMI\n['the', 'mind', 'blowing', 'licence', 'to', 'be', 'announced', 'by', '@ecomi_.', 'well', 'played', '@vevecollectible.', 'the', 'company', 'in', 'great', 'hands.', '#omi']\n['the', 'mind', 'blowing', 'licence', 'to', 'be', 'announced', 'by', '@ecomi_', 'well', 'played', '@vevecollectible', 'the', 'company', 'in', 'great', 'hands', '#omi'] \n---\nSergio Ramos set to be out for a month due to injury. #LFC\n['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury.', '#lfc']\n['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury', '#lfc'] \n---\nBayern Munich have confirmed Robert Lewandowski will be out for 4 weeks with a knee injury.\n['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.']\n['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.'] \n---\nSergio Aguero has been advised by his public relations team not to move to @ChelseaFC. \n\n@FCBarcelona are leading the race. Talks to start next week with the Spanish club.\n['sergio', 'aguero', 'has', 'been', 'advised', 'by', 'his', 'public', 'relations', 'team', 'not', 'to', 'move', 'to', '@chelseafc.', '@fcbarcelona', 'are', 'leading', 'the', 'race.', 'talks', 'to', 'start', 'next', 'week', 'with', 'the', 'spanish', 'club.']\n['sergio', 'aguero', 'has', 'been', 'advised', 'by', 'his', 'public', 'relations', 'team', 'not', 'to', 'move', 'to', '@chelseafc', '@fcbarcelona', 'are', 'leading', 'the', 'race', 'talks', 'to', 'start', 'next', 'week', 'with', 'the', 'spanish', 'club.'] \n---\nIf you want to make some extra money tomorrow give @BetcIub a follow! They have some huge Winners ðŸ’· #Ad https://t.co/MSgTiHMWdy\n['if', 'you', 'want', 'to', 'make', 'some', 'extra', 'money', 'tomorrow', 'give', '@betciub', 'a', 'follow!', 'they', 'have', 'some', 'huge', 'winners', 'ðŸ’·', '#ad', 'https://t.co/msgtihmwdy']\n['if', 'you', 'want', 'to', 'make', 'some', 'extra', 'money', 'tomorrow', 'give', '@betciub', 'a', 'follow', 'they', 'have', 'some', 'huge', 'winners', 'ðŸ’·', '#ad'] \n---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZRTny0gDR8a"
      },
      "source": [
        "# Corpora"
      ]
    },
    {
      "source": [
        "We define a class object to *stream* the corpus, by stream this means iterating through the corpus as a generator object. Streaming help us not to save corpora in RAM and make the programm memory-friendly. [**Gensim**](https://radimrehurek.com/gensim/index.html) python library deals with such corpus classes and allows you create documents on the fly.\n",
        "\n",
        "There's a simple and particular class defined. For now only `tweepy.API.user_timeline` method is available in this class. We can add some other [api wrapper's methods](https://docs.tweepy.org/en/v3.4.0/api.html#tweepy-api-twitter-api-wrapper) for various uses and initiate class object with them in the `tweepy.Cursor` object. Also we can control method's parameters (like `id` for `tweepy.API.user_timeline`) with the `tweepy.Cursor` object in the instantiation."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "class MyTexts:\n",
        "    \"\"\"Implement a class object to iterate on a specific user timeline.\n",
        "    This class define an iterator as a generator function\n",
        "    which yield cleaned text (removed links and punctuations) tokenized\"\"\"\n",
        "    def __init__(self, pagination_num=3):\n",
        "        self.pagination_num = pagination_num\n",
        "        # cursor on user's timeline\n",
        "        self.cursor = tw.Cursor(api.user_timeline, id=\"indykaila\",\n",
        "                              exclude_replies=True, include_rts=False,\n",
        "                              tweet_mode='extended').pages(self.pagination_num)\n",
        "    def __iter__(self):\n",
        "        for page in self.cursor:\n",
        "            for status in page:\n",
        "                # cleaning: removing links and some punctuations\n",
        "                link_removed = re.sub(r'\\bhttps:\\S+', '', status.full_text.lower())\n",
        "                punc_link_removed = re.sub(r'[â€“,-.!\":]\\D', ' ', link_removed)\n",
        "                yield punc_link_removed.split()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "32uHaNm9TTHJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object MyTexts.__iter__ at 0x7fe93084ea20>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "MyTexts().__iter__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVWYqLhpDgR9"
      },
      "source": [
        "> **Collect statistics about corpus and preprocess it**\n",
        "\n",
        "We can collect statistics about all tokens by creating **dictionaries** as a [`gensim.corpora.Dictionary` class object](https://radimrehurek.com/gensim/corpora/dictionary.html#module-gensim.corpora.dictionary). For *preprocessing* the corpus, we just remove *stop words* and *once words* from dictionary. Stopwords are the words which frequenctly will be used in sentences (like for, of , a, and, etc.) with no special semantics in this stage. Once words are the words which are used once in the corpus and can be ignored. For filtering these tokens we obtain their ids from dictionary and filter them out.\n",
        "\n",
        "With having dictionary object for a corpus, we can convert tokenized *documents* to vectors in **BoW representation**. Documents could be tweets, which here is considered so.\n",
        "\n",
        "Here we consider 100 first page of user's timeline as the whole corpus.\n",
        "\n",
        "At the end, we save the dictionary into disk for possible later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/saeedentezari/Documents/GitHub/milestone1-data-preparation/.venv/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n",
            "2021-04-04 04:49:08,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-04-04 04:54:59,862 : INFO : built Dictionary(3620 unique tokens: ['#cfc', 'albion', 'bromwich', 'callum', 'chelsea']...) from 1202 documents (total 19581 corpus positions)\n",
            "2021-04-04 04:54:59,889 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(3620 unique tokens: ['#cfc', 'albion', 'bromwich', 'callum', 'chelsea']...) from 1202 documents (total 19581 corpus positions)\", 'datetime': '2021-04-04T04:54:59.863224', 'gensim': '4.0.1', 'python': '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'created'}\n",
            "2021-04-04 04:54:59,896 : INFO : Dictionary lifecycle event {'fname_or_handle': 'dictionary.dict', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-04-04T04:54:59.896392', 'gensim': '4.0.1', 'python': '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'saving'}\n",
            "2021-04-04 04:54:59,900 : INFO : saved dictionary.dict\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora\n",
        "\n",
        "texts = MyTexts(100)  # define corpus streamable object\n",
        "# collect statistics about tokens into dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# preprocess: remove stop words and only once words from dictionary\n",
        "stop_words = set('for of a an and the to in on at with is are'.split())\n",
        "stop_word_ids = [dictionary.token2id[stopword]\n",
        "                 for stopword in stop_words\n",
        "                 if stopword in dictionary.token2id]\n",
        "once_word_ids = [tokenid\n",
        "                 for tokenid, docfreq in dictionary.dfs.items()\n",
        "                 if docfreq == 1]\n",
        "dictionary.filter_tokens(stop_word_ids + once_word_ids)\n",
        "dictionary.compactify()\n",
        "# store the dictionary\n",
        "dictionary.save('dictionary.dict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(1613 unique tokens: ['#cfc', 'callum', 'chelsea', 'disgusted', 'evening']...)\n"
          ]
        }
      ],
      "source": [
        "print(dictionary)"
      ]
    },
    {
      "source": [
        "With `dfs` attribute in `dictionary` object, you can see the *document frequencies*: token_id -> how many documents contain this token"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "dictionary.dfs"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{10: 12,\n",
              " 2: 17,\n",
              " 5: 2,\n",
              " 3: 2,\n",
              " 8: 4,\n",
              " 9: 5,\n",
              " 6: 9,\n",
              " 12: 119,\n",
              " 4: 3,\n",
              " 11: 3,\n",
              " 13: 13,\n",
              " 7: 26,\n",
              " 1: 3,\n",
              " 0: 43,\n",
              " 16: 18,\n",
              " 17: 6,\n",
              " 15: 29,\n",
              " 18: 22,\n",
              " 19: 5,\n",
              " 14: 132,\n",
              " 21: 47,\n",
              " 24: 122,\n",
              " 26: 151,\n",
              " 25: 6,\n",
              " 22: 4,\n",
              " 23: 6,\n",
              " 20: 258,\n",
              " 36: 7,\n",
              " 31: 2,\n",
              " 30: 137,\n",
              " 29: 5,\n",
              " 32: 48,\n",
              " 28: 4,\n",
              " 38: 10,\n",
              " 37: 10,\n",
              " 33: 2,\n",
              " 34: 10,\n",
              " 35: 2,\n",
              " 27: 4,\n",
              " 44: 10,\n",
              " 43: 3,\n",
              " 45: 19,\n",
              " 42: 70,\n",
              " 41: 7,\n",
              " 39: 24,\n",
              " 40: 43,\n",
              " 47: 7,\n",
              " 50: 3,\n",
              " 48: 135,\n",
              " 51: 2,\n",
              " 46: 10,\n",
              " 52: 15,\n",
              " 49: 10,\n",
              " 56: 6,\n",
              " 59: 165,\n",
              " 57: 75,\n",
              " 55: 3,\n",
              " 60: 85,\n",
              " 64: 3,\n",
              " 69: 54,\n",
              " 63: 79,\n",
              " 61: 19,\n",
              " 53: 25,\n",
              " 54: 7,\n",
              " 65: 4,\n",
              " 68: 21,\n",
              " 67: 20,\n",
              " 62: 31,\n",
              " 70: 21,\n",
              " 66: 3,\n",
              " 58: 3,\n",
              " 76: 60,\n",
              " 83: 58,\n",
              " 82: 58,\n",
              " 77: 39,\n",
              " 79: 24,\n",
              " 73: 9,\n",
              " 78: 16,\n",
              " 81: 23,\n",
              " 75: 19,\n",
              " 72: 11,\n",
              " 74: 21,\n",
              " 80: 64,\n",
              " 84: 3,\n",
              " 71: 15,\n",
              " 89: 66,\n",
              " 85: 32,\n",
              " 87: 9,\n",
              " 88: 18,\n",
              " 86: 51,\n",
              " 90: 19,\n",
              " 96: 10,\n",
              " 94: 2,\n",
              " 95: 4,\n",
              " 91: 48,\n",
              " 93: 40,\n",
              " 99: 35,\n",
              " 102: 37,\n",
              " 97: 4,\n",
              " 101: 2,\n",
              " 98: 71,\n",
              " 92: 13,\n",
              " 104: 20,\n",
              " 100: 12,\n",
              " 103: 6,\n",
              " 109: 3,\n",
              " 111: 2,\n",
              " 112: 3,\n",
              " 105: 40,\n",
              " 120: 46,\n",
              " 107: 70,\n",
              " 114: 16,\n",
              " 116: 3,\n",
              " 115: 18,\n",
              " 117: 8,\n",
              " 110: 42,\n",
              " 121: 12,\n",
              " 118: 2,\n",
              " 119: 20,\n",
              " 122: 3,\n",
              " 108: 2,\n",
              " 113: 13,\n",
              " 106: 2,\n",
              " 126: 3,\n",
              " 124: 12,\n",
              " 123: 60,\n",
              " 125: 26,\n",
              " 127: 40,\n",
              " 130: 118,\n",
              " 134: 8,\n",
              " 136: 26,\n",
              " 132: 40,\n",
              " 131: 10,\n",
              " 129: 26,\n",
              " 135: 37,\n",
              " 133: 4,\n",
              " 128: 6,\n",
              " 140: 3,\n",
              " 141: 11,\n",
              " 143: 21,\n",
              " 139: 14,\n",
              " 138: 6,\n",
              " 137: 10,\n",
              " 142: 5,\n",
              " 148: 76,\n",
              " 147: 83,\n",
              " 145: 2,\n",
              " 146: 62,\n",
              " 144: 3,\n",
              " 151: 7,\n",
              " 149: 17,\n",
              " 153: 2,\n",
              " 152: 9,\n",
              " 150: 20,\n",
              " 155: 21,\n",
              " 156: 4,\n",
              " 154: 36,\n",
              " 157: 2,\n",
              " 166: 92,\n",
              " 159: 2,\n",
              " 164: 6,\n",
              " 163: 4,\n",
              " 160: 28,\n",
              " 158: 8,\n",
              " 165: 3,\n",
              " 161: 11,\n",
              " 162: 11,\n",
              " 172: 10,\n",
              " 169: 10,\n",
              " 174: 5,\n",
              " 171: 48,\n",
              " 175: 59,\n",
              " 168: 6,\n",
              " 170: 104,\n",
              " 176: 12,\n",
              " 173: 26,\n",
              " 167: 12,\n",
              " 177: 11,\n",
              " 178: 14,\n",
              " 181: 3,\n",
              " 182: 2,\n",
              " 180: 22,\n",
              " 179: 3,\n",
              " 193: 19,\n",
              " 185: 9,\n",
              " 192: 9,\n",
              " 195: 6,\n",
              " 183: 11,\n",
              " 186: 15,\n",
              " 189: 6,\n",
              " 188: 20,\n",
              " 191: 52,\n",
              " 187: 11,\n",
              " 184: 57,\n",
              " 197: 7,\n",
              " 194: 6,\n",
              " 196: 8,\n",
              " 190: 10,\n",
              " 203: 13,\n",
              " 206: 9,\n",
              " 211: 41,\n",
              " 200: 7,\n",
              " 205: 3,\n",
              " 212: 36,\n",
              " 213: 2,\n",
              " 204: 10,\n",
              " 208: 2,\n",
              " 214: 20,\n",
              " 199: 27,\n",
              " 218: 2,\n",
              " 217: 2,\n",
              " 216: 5,\n",
              " 209: 12,\n",
              " 201: 3,\n",
              " 198: 2,\n",
              " 215: 9,\n",
              " 202: 10,\n",
              " 210: 7,\n",
              " 207: 28,\n",
              " 230: 2,\n",
              " 221: 2,\n",
              " 233: 6,\n",
              " 225: 6,\n",
              " 231: 27,\n",
              " 220: 22,\n",
              " 226: 4,\n",
              " 227: 12,\n",
              " 235: 6,\n",
              " 222: 47,\n",
              " 229: 28,\n",
              " 224: 8,\n",
              " 219: 90,\n",
              " 223: 13,\n",
              " 232: 3,\n",
              " 234: 3,\n",
              " 228: 15,\n",
              " 240: 12,\n",
              " 236: 5,\n",
              " 239: 30,\n",
              " 238: 6,\n",
              " 237: 38,\n",
              " 247: 62,\n",
              " 242: 9,\n",
              " 245: 5,\n",
              " 243: 8,\n",
              " 244: 89,\n",
              " 246: 28,\n",
              " 241: 2,\n",
              " 248: 16,\n",
              " 249: 2,\n",
              " 254: 32,\n",
              " 252: 3,\n",
              " 250: 16,\n",
              " 251: 3,\n",
              " 253: 6,\n",
              " 260: 9,\n",
              " 257: 19,\n",
              " 258: 6,\n",
              " 256: 10,\n",
              " 255: 73,\n",
              " 259: 4,\n",
              " 264: 7,\n",
              " 261: 8,\n",
              " 263: 4,\n",
              " 262: 24,\n",
              " 268: 3,\n",
              " 265: 2,\n",
              " 267: 3,\n",
              " 266: 3,\n",
              " 269: 12,\n",
              " 274: 4,\n",
              " 270: 3,\n",
              " 273: 12,\n",
              " 275: 17,\n",
              " 271: 6,\n",
              " 272: 3,\n",
              " 276: 2,\n",
              " 278: 3,\n",
              " 286: 18,\n",
              " 285: 26,\n",
              " 284: 4,\n",
              " 281: 9,\n",
              " 280: 3,\n",
              " 279: 9,\n",
              " 277: 6,\n",
              " 282: 2,\n",
              " 283: 9,\n",
              " 288: 16,\n",
              " 287: 18,\n",
              " 289: 3,\n",
              " 290: 20,\n",
              " 291: 10,\n",
              " 292: 4,\n",
              " 293: 6,\n",
              " 294: 5,\n",
              " 295: 7,\n",
              " 297: 11,\n",
              " 296: 7,\n",
              " 299: 5,\n",
              " 306: 7,\n",
              " 300: 4,\n",
              " 304: 6,\n",
              " 301: 7,\n",
              " 302: 10,\n",
              " 307: 20,\n",
              " 305: 10,\n",
              " 303: 3,\n",
              " 298: 24,\n",
              " 309: 10,\n",
              " 308: 22,\n",
              " 311: 16,\n",
              " 310: 18,\n",
              " 312: 2,\n",
              " 313: 9,\n",
              " 315: 3,\n",
              " 314: 2,\n",
              " 319: 13,\n",
              " 318: 5,\n",
              " 317: 5,\n",
              " 316: 2,\n",
              " 322: 4,\n",
              " 321: 4,\n",
              " 320: 67,\n",
              " 327: 17,\n",
              " 323: 2,\n",
              " 325: 2,\n",
              " 324: 68,\n",
              " 326: 30,\n",
              " 328: 12,\n",
              " 329: 6,\n",
              " 331: 4,\n",
              " 333: 6,\n",
              " 330: 24,\n",
              " 334: 7,\n",
              " 332: 5,\n",
              " 335: 6,\n",
              " 336: 8,\n",
              " 345: 5,\n",
              " 344: 4,\n",
              " 340: 3,\n",
              " 338: 33,\n",
              " 347: 7,\n",
              " 348: 16,\n",
              " 346: 11,\n",
              " 342: 42,\n",
              " 343: 15,\n",
              " 337: 8,\n",
              " 339: 7,\n",
              " 341: 2,\n",
              " 357: 13,\n",
              " 352: 2,\n",
              " 355: 4,\n",
              " 353: 4,\n",
              " 356: 4,\n",
              " 359: 31,\n",
              " 358: 5,\n",
              " 354: 7,\n",
              " 351: 8,\n",
              " 350: 27,\n",
              " 349: 30,\n",
              " 362: 26,\n",
              " 363: 3,\n",
              " 360: 13,\n",
              " 361: 4,\n",
              " 368: 3,\n",
              " 364: 8,\n",
              " 367: 4,\n",
              " 365: 4,\n",
              " 366: 17,\n",
              " 370: 19,\n",
              " 369: 16,\n",
              " 374: 4,\n",
              " 375: 2,\n",
              " 373: 42,\n",
              " 371: 31,\n",
              " 372: 3,\n",
              " 377: 15,\n",
              " 376: 10,\n",
              " 378: 5,\n",
              " 380: 8,\n",
              " 379: 4,\n",
              " 381: 2,\n",
              " 382: 8,\n",
              " 385: 9,\n",
              " 384: 13,\n",
              " 383: 27,\n",
              " 390: 5,\n",
              " 388: 2,\n",
              " 386: 3,\n",
              " 394: 2,\n",
              " 387: 7,\n",
              " 391: 21,\n",
              " 392: 31,\n",
              " 393: 2,\n",
              " 389: 4,\n",
              " 403: 4,\n",
              " 395: 5,\n",
              " 396: 3,\n",
              " 397: 3,\n",
              " 401: 3,\n",
              " 399: 4,\n",
              " 402: 5,\n",
              " 400: 6,\n",
              " 398: 6,\n",
              " 404: 4,\n",
              " 406: 3,\n",
              " 405: 2,\n",
              " 409: 22,\n",
              " 418: 22,\n",
              " 416: 3,\n",
              " 417: 19,\n",
              " 411: 3,\n",
              " 412: 5,\n",
              " 413: 12,\n",
              " 408: 5,\n",
              " 415: 6,\n",
              " 407: 14,\n",
              " 410: 49,\n",
              " 414: 2,\n",
              " 419: 36,\n",
              " 420: 8,\n",
              " 421: 5,\n",
              " 425: 21,\n",
              " 423: 10,\n",
              " 424: 11,\n",
              " 422: 2,\n",
              " 429: 3,\n",
              " 428: 23,\n",
              " 427: 6,\n",
              " 430: 5,\n",
              " 426: 7,\n",
              " 431: 5,\n",
              " 432: 2,\n",
              " 433: 9,\n",
              " 434: 8,\n",
              " 435: 28,\n",
              " 442: 11,\n",
              " 438: 12,\n",
              " 439: 10,\n",
              " 436: 18,\n",
              " 443: 6,\n",
              " 441: 29,\n",
              " 440: 33,\n",
              " 437: 18,\n",
              " 447: 3,\n",
              " 446: 3,\n",
              " 445: 8,\n",
              " 444: 4,\n",
              " 448: 4,\n",
              " 453: 3,\n",
              " 452: 4,\n",
              " 449: 7,\n",
              " 450: 3,\n",
              " 451: 13,\n",
              " 454: 2,\n",
              " 455: 8,\n",
              " 458: 5,\n",
              " 456: 5,\n",
              " 457: 10,\n",
              " 460: 7,\n",
              " 459: 8,\n",
              " 461: 29,\n",
              " 462: 39,\n",
              " 463: 3,\n",
              " 467: 10,\n",
              " 465: 12,\n",
              " 466: 2,\n",
              " 464: 14,\n",
              " 475: 3,\n",
              " 470: 11,\n",
              " 474: 6,\n",
              " 468: 3,\n",
              " 476: 8,\n",
              " 473: 2,\n",
              " 469: 20,\n",
              " 471: 5,\n",
              " 472: 12,\n",
              " 482: 3,\n",
              " 480: 23,\n",
              " 484: 11,\n",
              " 478: 2,\n",
              " 477: 3,\n",
              " 481: 3,\n",
              " 483: 8,\n",
              " 479: 2,\n",
              " 494: 6,\n",
              " 495: 4,\n",
              " 486: 35,\n",
              " 485: 2,\n",
              " 488: 8,\n",
              " 493: 16,\n",
              " 491: 6,\n",
              " 489: 3,\n",
              " 490: 14,\n",
              " 487: 10,\n",
              " 492: 2,\n",
              " 500: 5,\n",
              " 499: 2,\n",
              " 496: 3,\n",
              " 498: 6,\n",
              " 502: 2,\n",
              " 501: 2,\n",
              " 497: 2,\n",
              " 503: 24,\n",
              " 504: 3,\n",
              " 505: 7,\n",
              " 506: 3,\n",
              " 507: 6,\n",
              " 509: 4,\n",
              " 508: 2,\n",
              " 510: 3,\n",
              " 511: 46,\n",
              " 516: 3,\n",
              " 514: 5,\n",
              " 512: 38,\n",
              " 513: 28,\n",
              " 515: 5,\n",
              " 517: 2,\n",
              " 518: 8,\n",
              " 519: 4,\n",
              " 522: 4,\n",
              " 523: 5,\n",
              " 520: 8,\n",
              " 524: 6,\n",
              " 521: 5,\n",
              " 525: 13,\n",
              " 527: 3,\n",
              " 528: 24,\n",
              " 526: 3,\n",
              " 529: 2,\n",
              " 530: 8,\n",
              " 534: 8,\n",
              " 535: 4,\n",
              " 532: 24,\n",
              " 531: 2,\n",
              " 533: 3,\n",
              " 538: 10,\n",
              " 537: 2,\n",
              " 539: 6,\n",
              " 536: 7,\n",
              " 544: 31,\n",
              " 543: 3,\n",
              " 546: 2,\n",
              " 540: 2,\n",
              " 542: 5,\n",
              " 545: 9,\n",
              " 541: 3,\n",
              " 547: 14,\n",
              " 550: 4,\n",
              " 548: 6,\n",
              " 549: 5,\n",
              " 551: 10,\n",
              " 553: 11,\n",
              " 554: 14,\n",
              " 552: 3,\n",
              " 556: 2,\n",
              " 555: 7,\n",
              " 560: 3,\n",
              " 558: 9,\n",
              " 557: 9,\n",
              " 559: 6,\n",
              " 561: 6,\n",
              " 562: 4,\n",
              " 564: 6,\n",
              " 563: 9,\n",
              " 565: 9,\n",
              " 566: 3,\n",
              " 569: 3,\n",
              " 571: 7,\n",
              " 568: 14,\n",
              " 570: 7,\n",
              " 567: 4,\n",
              " 573: 7,\n",
              " 576: 5,\n",
              " 574: 8,\n",
              " 578: 2,\n",
              " 577: 9,\n",
              " 572: 9,\n",
              " 575: 3,\n",
              " 580: 2,\n",
              " 579: 25,\n",
              " 582: 2,\n",
              " 581: 2,\n",
              " 583: 2,\n",
              " 584: 8,\n",
              " 585: 8,\n",
              " 586: 2,\n",
              " 589: 7,\n",
              " 588: 6,\n",
              " 590: 4,\n",
              " 587: 5,\n",
              " 592: 5,\n",
              " 591: 3,\n",
              " 594: 9,\n",
              " 593: 3,\n",
              " 595: 16,\n",
              " 597: 3,\n",
              " 599: 7,\n",
              " 600: 2,\n",
              " 601: 2,\n",
              " 598: 2,\n",
              " 596: 2,\n",
              " 604: 22,\n",
              " 606: 7,\n",
              " 603: 7,\n",
              " 605: 4,\n",
              " 602: 14,\n",
              " 607: 3,\n",
              " 609: 10,\n",
              " 608: 5,\n",
              " 610: 5,\n",
              " 612: 4,\n",
              " 611: 2,\n",
              " 614: 7,\n",
              " 613: 21,\n",
              " 615: 4,\n",
              " 619: 5,\n",
              " 616: 15,\n",
              " 618: 3,\n",
              " 617: 4,\n",
              " 623: 10,\n",
              " 620: 3,\n",
              " 621: 7,\n",
              " 622: 9,\n",
              " 625: 7,\n",
              " 624: 34,\n",
              " 632: 6,\n",
              " 626: 10,\n",
              " 629: 3,\n",
              " 633: 6,\n",
              " 628: 3,\n",
              " 631: 21,\n",
              " 630: 2,\n",
              " 627: 5,\n",
              " 634: 2,\n",
              " 636: 2,\n",
              " 638: 2,\n",
              " 635: 2,\n",
              " 637: 3,\n",
              " 640: 21,\n",
              " 639: 2,\n",
              " 642: 2,\n",
              " 641: 13,\n",
              " 643: 6,\n",
              " 645: 4,\n",
              " 644: 3,\n",
              " 647: 4,\n",
              " 648: 4,\n",
              " 646: 3,\n",
              " 651: 2,\n",
              " 652: 2,\n",
              " 650: 2,\n",
              " 649: 4,\n",
              " 654: 8,\n",
              " 653: 6,\n",
              " 659: 15,\n",
              " 663: 3,\n",
              " 662: 7,\n",
              " 656: 17,\n",
              " 655: 3,\n",
              " 657: 4,\n",
              " 658: 8,\n",
              " 660: 15,\n",
              " 661: 3,\n",
              " 665: 6,\n",
              " 666: 2,\n",
              " 664: 11,\n",
              " 668: 3,\n",
              " 667: 24,\n",
              " 673: 8,\n",
              " 677: 2,\n",
              " 674: 3,\n",
              " 675: 3,\n",
              " 672: 2,\n",
              " 678: 8,\n",
              " 669: 5,\n",
              " 676: 2,\n",
              " 670: 2,\n",
              " 679: 22,\n",
              " 671: 3,\n",
              " 681: 2,\n",
              " 680: 10,\n",
              " 682: 3,\n",
              " 683: 5,\n",
              " 684: 17,\n",
              " 685: 4,\n",
              " 687: 14,\n",
              " 689: 2,\n",
              " 690: 4,\n",
              " 688: 2,\n",
              " 686: 2,\n",
              " 693: 4,\n",
              " 691: 3,\n",
              " 692: 2,\n",
              " 694: 4,\n",
              " 697: 8,\n",
              " 695: 7,\n",
              " 696: 10,\n",
              " 698: 15,\n",
              " 699: 2,\n",
              " 705: 34,\n",
              " 704: 2,\n",
              " 700: 4,\n",
              " 702: 3,\n",
              " 703: 5,\n",
              " 701: 5,\n",
              " 706: 2,\n",
              " 707: 6,\n",
              " 708: 6,\n",
              " 709: 15,\n",
              " 711: 11,\n",
              " 710: 2,\n",
              " 715: 8,\n",
              " 712: 2,\n",
              " 718: 2,\n",
              " 716: 10,\n",
              " 713: 2,\n",
              " 714: 5,\n",
              " 717: 2,\n",
              " 720: 4,\n",
              " 719: 8,\n",
              " 725: 5,\n",
              " 722: 2,\n",
              " 729: 22,\n",
              " 727: 2,\n",
              " 726: 14,\n",
              " 721: 2,\n",
              " 723: 5,\n",
              " 730: 3,\n",
              " 728: 12,\n",
              " 724: 3,\n",
              " 734: 4,\n",
              " 735: 17,\n",
              " 732: 2,\n",
              " 731: 3,\n",
              " 733: 6,\n",
              " 738: 4,\n",
              " 736: 2,\n",
              " 737: 23,\n",
              " 741: 7,\n",
              " 742: 30,\n",
              " 740: 23,\n",
              " 739: 7,\n",
              " 743: 4,\n",
              " 746: 3,\n",
              " 745: 2,\n",
              " 744: 2,\n",
              " 748: 2,\n",
              " 747: 2,\n",
              " 750: 2,\n",
              " 749: 4,\n",
              " 751: 3,\n",
              " 752: 14,\n",
              " 753: 2,\n",
              " 754: 6,\n",
              " 755: 7,\n",
              " 756: 3,\n",
              " 760: 4,\n",
              " 761: 3,\n",
              " 759: 2,\n",
              " 758: 12,\n",
              " 757: 15,\n",
              " 762: 10,\n",
              " 763: 7,\n",
              " 765: 3,\n",
              " 766: 9,\n",
              " 764: 4,\n",
              " 767: 10,\n",
              " 768: 3,\n",
              " 769: 2,\n",
              " 772: 5,\n",
              " 771: 2,\n",
              " 770: 2,\n",
              " 773: 2,\n",
              " 774: 5,\n",
              " 780: 16,\n",
              " 781: 7,\n",
              " 778: 13,\n",
              " 777: 2,\n",
              " 775: 2,\n",
              " 776: 2,\n",
              " 779: 21,\n",
              " 782: 8,\n",
              " 783: 9,\n",
              " 784: 5,\n",
              " 791: 4,\n",
              " 787: 7,\n",
              " 789: 4,\n",
              " 785: 5,\n",
              " 790: 2,\n",
              " 788: 4,\n",
              " 786: 10,\n",
              " 792: 3,\n",
              " 793: 6,\n",
              " 794: 6,\n",
              " 795: 7,\n",
              " 796: 4,\n",
              " 799: 3,\n",
              " 798: 7,\n",
              " 797: 2,\n",
              " 801: 10,\n",
              " 800: 2,\n",
              " 802: 3,\n",
              " 803: 3,\n",
              " 804: 2,\n",
              " 805: 7,\n",
              " 808: 3,\n",
              " 807: 2,\n",
              " 806: 3,\n",
              " 809: 8,\n",
              " 810: 24,\n",
              " 811: 2,\n",
              " 812: 3,\n",
              " 813: 13,\n",
              " 814: 4,\n",
              " 816: 5,\n",
              " 815: 3,\n",
              " 817: 3,\n",
              " 819: 4,\n",
              " 818: 8,\n",
              " 822: 2,\n",
              " 821: 8,\n",
              " 823: 8,\n",
              " 820: 28,\n",
              " 825: 3,\n",
              " 828: 5,\n",
              " 829: 2,\n",
              " 830: 4,\n",
              " 826: 2,\n",
              " 827: 4,\n",
              " 824: 3,\n",
              " 831: 3,\n",
              " 838: 2,\n",
              " 836: 4,\n",
              " 834: 7,\n",
              " 832: 5,\n",
              " 837: 2,\n",
              " 835: 2,\n",
              " 833: 2,\n",
              " 841: 6,\n",
              " 840: 5,\n",
              " 842: 2,\n",
              " 843: 2,\n",
              " 839: 9,\n",
              " 845: 2,\n",
              " 846: 3,\n",
              " 847: 3,\n",
              " 844: 2,\n",
              " 848: 6,\n",
              " 849: 3,\n",
              " 850: 2,\n",
              " 854: 8,\n",
              " 853: 15,\n",
              " 851: 8,\n",
              " 855: 3,\n",
              " 852: 3,\n",
              " 859: 11,\n",
              " 858: 2,\n",
              " 856: 3,\n",
              " 864: 4,\n",
              " 861: 3,\n",
              " 857: 5,\n",
              " 860: 10,\n",
              " 863: 2,\n",
              " 862: 2,\n",
              " 865: 3,\n",
              " 868: 2,\n",
              " 867: 5,\n",
              " 866: 3,\n",
              " 871: 14,\n",
              " 872: 3,\n",
              " 869: 14,\n",
              " 870: 19,\n",
              " 873: 2,\n",
              " 874: 7,\n",
              " 875: 2,\n",
              " 876: 6,\n",
              " 879: 3,\n",
              " 877: 2,\n",
              " 878: 4,\n",
              " 881: 5,\n",
              " 883: 3,\n",
              " 884: 2,\n",
              " 880: 2,\n",
              " 886: 2,\n",
              " 882: 3,\n",
              " 885: 3,\n",
              " 888: 3,\n",
              " 887: 4,\n",
              " 889: 5,\n",
              " 892: 5,\n",
              " 890: 5,\n",
              " 893: 2,\n",
              " 891: 4,\n",
              " 894: 3,\n",
              " 896: 2,\n",
              " 895: 5,\n",
              " 897: 2,\n",
              " 900: 3,\n",
              " 901: 3,\n",
              " 898: 2,\n",
              " 899: 5,\n",
              " 902: 2,\n",
              " 906: 5,\n",
              " 904: 3,\n",
              " 903: 4,\n",
              " 905: 2,\n",
              " 907: 5,\n",
              " 908: 2,\n",
              " 909: 2,\n",
              " 910: 2,\n",
              " 911: 2,\n",
              " 912: 5,\n",
              " 913: 4,\n",
              " 915: 4,\n",
              " 914: 3,\n",
              " 916: 6,\n",
              " 917: 6,\n",
              " 921: 12,\n",
              " 918: 10,\n",
              " 919: 4,\n",
              " 920: 9,\n",
              " 922: 9,\n",
              " 923: 2,\n",
              " 924: 9,\n",
              " 925: 4,\n",
              " 927: 8,\n",
              " 926: 3,\n",
              " 928: 2,\n",
              " 929: 5,\n",
              " 930: 5,\n",
              " 931: 2,\n",
              " 932: 10,\n",
              " 933: 5,\n",
              " 934: 2,\n",
              " 937: 13,\n",
              " 935: 3,\n",
              " 939: 6,\n",
              " 936: 5,\n",
              " 938: 8,\n",
              " 940: 3,\n",
              " 941: 9,\n",
              " 942: 5,\n",
              " 943: 4,\n",
              " 944: 2,\n",
              " 946: 2,\n",
              " 947: 10,\n",
              " 945: 2,\n",
              " 948: 2,\n",
              " 949: 2,\n",
              " 953: 2,\n",
              " 951: 8,\n",
              " 955: 2,\n",
              " 952: 2,\n",
              " 950: 4,\n",
              " 954: 2,\n",
              " 956: 3,\n",
              " 957: 5,\n",
              " 958: 9,\n",
              " 959: 7,\n",
              " 960: 3,\n",
              " 961: 7,\n",
              " 962: 5,\n",
              " 963: 2,\n",
              " 964: 2,\n",
              " 965: 2,\n",
              " 966: 2,\n",
              " 967: 2,\n",
              " 974: 4,\n",
              " 968: 2,\n",
              " 970: 8,\n",
              " 971: 3,\n",
              " 969: 2,\n",
              " 972: 9,\n",
              " 973: 2,\n",
              " 975: 3,\n",
              " 977: 9,\n",
              " 976: 14,\n",
              " 978: 2,\n",
              " 980: 2,\n",
              " 979: 3,\n",
              " 981: 6,\n",
              " 982: 2,\n",
              " 983: 6,\n",
              " 984: 2,\n",
              " 986: 7,\n",
              " 985: 2,\n",
              " 987: 3,\n",
              " 988: 10,\n",
              " 989: 2,\n",
              " 991: 6,\n",
              " 993: 5,\n",
              " 992: 6,\n",
              " 990: 16,\n",
              " 994: 8,\n",
              " 995: 2,\n",
              " 996: 6,\n",
              " 1001: 2,\n",
              " 1000: 9,\n",
              " 997: 9,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "source": [
        "> **Building training corpus**\n",
        "\n",
        "Now that we have a dictionary, we can select our training corpus and vectorize it's documents in BoW representation. For this purpose, I choosed 10 first page of user's timeline for training corpus, building a generator to yield `dictionary.doc2bow(text)` for each `text` (= tweet) in `train_text` (= training corpus as texts), and store training corpus in BoW (= `train_corpus_bow`) to a file named 'train_corpus_bow.mm' with `gensim.corpora.MmCorpus.serialize` method in [`gensim.corpora.MmCorpus` class](https://radimrehurek.com/gensim/corpora/mmcorpus.html#module-gensim.corpora.mmcorpus). Corpus serialized using the sparse coordinate [Matrix Market (.mm) format](https://math.nist.gov/MatrixMarket/formats.html).\n",
        "\n",
        "> **Note**: Every document's token that is not in the dictionary is a \"blah\" for `gensim.corpora.Dictionary.doc2bow` method and would'nt count in document's BoW representation."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 04:56:15,345 : INFO : storing corpus in Matrix Market format to train_corpus_bow.mm\n",
            "2021-04-04 04:56:15,372 : INFO : saving sparse matrix to train_corpus_bow.mm\n",
            "2021-04-04 04:56:18,804 : INFO : PROGRESS: saving document #0\n",
            "2021-04-04 04:56:50,968 : INFO : saved 104x572 matrix, density=1.952% (1161/59488)\n",
            "2021-04-04 04:56:50,970 : INFO : saving MmCorpus index to train_corpus_bow.mm.index\n"
          ]
        }
      ],
      "source": [
        "# choose 10 first page of user's timeline for training corpus\n",
        "train_texts = MyTexts(10)\n",
        "# stream on the training corpus and store it in bow representation\n",
        "train_corpus_bow = (dictionary.doc2bow(text) for text in train_texts)\n",
        "corpora.MmCorpus.serialize('train_corpus_bow.mm', train_corpus_bow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu0FZthg1uDE",
        "tags": []
      },
      "source": [
        "train_texts = MyTexts(10)\n",
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')\n",
        "for count, (tweet_bow, tweet_text) in enumerate(zip(train_corpus_bow, train_texts)):\n",
        "    print(tweet_bow, tweet_text, '\\n---')\n",
        "    if count == 5: break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 04:57:32,765 : INFO : loaded corpus index from train_corpus_bow.mm.index\n",
            "2021-04-04 04:57:32,774 : INFO : initializing cython corpus reader from train_corpus_bow.mm\n",
            "2021-04-04 04:57:32,788 : INFO : accepted corpus with 104 documents, 572 features, 1161 non-zero entries\n",
            "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0)] ['statement', 'chelsea', 'fc', 'is', 'disgusted', 'with', 'posts', 'on', 'social', 'media', 'this', 'evening', 'targeting', 'west', 'bromwich', 'albion', 'player', 'callum', 'robinson', '#cfc'] \n",
            "---\n",
            "[(14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0)] ['david', 'luiz', 'could', 'need', 'surgery', '#afc'] \n",
            "---\n",
            "[(20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0)] ['confirmed', 'liverpool', 'will', 'meet', 'with', 'erling', \"haaland's\", 'representatives', 'on', 'friday', '#lfc'] \n",
            "---\n",
            "[(27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0)] ['the', 'mind', 'blowing', 'licence', 'to', 'be', 'announced', 'by', '@ecomi_', 'well', 'played', '@vevecollectible', 'the', 'company', 'in', 'great', 'hands', '#omi'] \n",
            "---\n",
            "[(20, 1.0), (30, 1.0), (39, 1.0), (40, 1.0), (41, 1.0), (42, 1.0), (43, 1.0), (44, 1.0), (45, 1.0)] ['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury', '#lfc'] \n",
            "---\n",
            "[(21, 1.0), (26, 1.0), (30, 1.0), (42, 1.0), (46, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 1.0), (52, 1.0)] ['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.'] \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLN3MOSAC43C"
      },
      "source": [
        "# Train the Models with Training Corpus"
      ]
    },
    {
      "source": [
        "Now that we have BoW representation of training corpus (in `train_corpus_bow`), we can *transform* it to have various representations. These tranformations would be obtained by training the *models* like **TF-IDF** and **LSI**. **Gensim** have a module `gensim.models` which contains various models such as [`gensim.models.TfidfModel`](https://radimrehurek.com/gensim/models/tfidfmodel.html#module-gensim.models.tfidfmodel) and [`gensim.models.LsiModel`](https://radimrehurek.com/gensim/models/lsimodel.html#module-gensim.models.lsimodel) class objects."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "First we load training corpus in BoW to be sure."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 04:57:45,136 : INFO : loaded corpus index from train_corpus_bow.mm.index\n",
            "2021-04-04 04:57:45,159 : INFO : initializing cython corpus reader from train_corpus_bow.mm\n",
            "2021-04-04 04:57:45,181 : INFO : accepted corpus with 104 documents, 572 features, 1161 non-zero entries\n"
          ]
        }
      ],
      "source": [
        "# load training corpus in bow from disk\n",
        "train_corpus_bow = corpora.MmCorpus('train_corpus_bow.mm')"
      ]
    },
    {
      "source": [
        "Then we can initialize a tfidf model with the corpus in BoW. A TF-IDF model would be *trained* with this training corpus."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 04:57:56,113 : INFO : collecting document frequencies\n",
            "2021-04-04 04:57:56,125 : INFO : PROGRESS: processing document #0\n",
            "2021-04-04 04:57:56,172 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 104 documents and 572 features (1161 matrix non-zeros)', 'datetime': '2021-04-04T04:57:56.172322', 'gensim': '4.0.1', 'python': '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'initialize'}\n"
          ]
        }
      ],
      "source": [
        "from gensim import models\n",
        "\n",
        "# initialize a tfidf model with training corpus in bow: training\n",
        "tfidf_model = models.TfidfModel(train_corpus_bow)"
      ]
    },
    {
      "source": [
        "Now that you have a TF-IDF model, you can transform any documents (= wheter it's a tweet or the corpus as a whole, just everything in BoW representation) with only indexing it into the `tfidf_model` object and get the transformed vectors (= documents) in the output. So TF-IDF model transforms documents from BoW representation into Tfidf representation, this can be summarized in the notation *bow->tfidf*.\n",
        "\n",
        "> **Note**: Tranformation could be done just **after** training the model. In fact, training the model gives us a transformation model.\n",
        "\n",
        "Here I transform (bow->tfidf) the whole training corpus because I want to give the whole training corpus in Tfidf to another model for training."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tranform training corpus bow->tfidf\n",
        "train_corpus_tfidf = tfidf_model[train_corpus_bow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.20305576652385546), (1, 0.2894543924822621), (2, 0.2209851013533081), (3, 0.2894543924822621), (4, 0.2894543924822621), (5, 0.2894543924822621), (6, 0.2894543924822621), (7, 0.2209851013533081), (8, 0.2894543924822621), (9, 0.2894543924822621), (10, 0.2894543924822621), (11, 0.2894543924822621), (12, 0.15251581022435406), (13, 0.2894543924822621)] ['statement', 'chelsea', 'fc', 'is', 'disgusted', 'with', 'posts', 'on', 'social', 'media', 'this', 'evening', 'targeting', 'west', 'bromwich', 'albion', 'player', 'callum', 'robinson', '#cfc'] \n---\n[(14, 0.2325356183336032), (15, 0.39234888686124225), (16, 0.4611767152851847), (17, 0.4611767152851847), (18, 0.39234888686124225), (19, 0.4611767152851847)] ['david', 'luiz', 'could', 'need', 'surgery', '#afc'] \n---\n[(20, 0.17086465762202846), (21, 0.35364733935246695), (22, 0.4604179485908612), (23, 0.541186796496686), (24, 0.2851556061823202), (25, 0.4604179485908612), (26, 0.2181114048733865)] ['confirmed', 'liverpool', 'will', 'meet', 'with', 'erling', \"haaland's\", 'representatives', 'on', 'friday', '#lfc'] \n---\n[(27, 0.24896145624540816), (28, 0.24896145624540816), (29, 0.3548925908516259), (30, 0.12597921746267218), (31, 0.301927023548517), (32, 0.16501301824055797), (33, 0.3548925908516259), (34, 0.3548925908516259), (35, 0.3548925908516259), (36, 0.2709441528467757), (37, 0.301927023548517), (38, 0.2709441528467757)] ['the', 'mind', 'blowing', 'licence', 'to', 'be', 'announced', 'by', '@ecomi_', 'well', 'played', '@vevecollectible', 'the', 'company', 'in', 'great', 'hands', '#omi'] \n---\n[(20, 0.1441938012473824), (30, 0.16212258371013036), (39, 0.34867787707672604), (40, 0.38854971586165526), (41, 0.4567110744714364), (42, 0.24064467968201572), (43, 0.4567110744714364), (44, 0.32038835725187403), (45, 0.32038835725187403)] ['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury', '#lfc'] \n---\n[(21, 0.2529354569869149), (26, 0.15599751992117789), (30, 0.13740055437589693), (42, 0.20394883698027955), (46, 0.38706732514321374), (47, 0.3292998738377048), (48, 0.18722477756884565), (49, 0.38706732514321374), (50, 0.38706732514321374), (51, 0.38706732514321374), (52, 0.3292998738377048)] ['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.'] \n---\n"
          ]
        }
      ],
      "source": [
        "for count, (tweet_tfidf, tweet_text) in enumerate(zip(train_corpus_tfidf, MyTexts(10))):\n",
        "    print(tweet_tfidf, tweet_text, '\\n---')\n",
        "    if count == 5: break"
      ]
    },
    {
      "source": [
        "Now that's time to give a brief description about BoW and Tfidf representations.\n",
        "\n",
        "first of all a dictionary should be created.\n",
        "\n",
        "> **BoW representation**: Any documents can have a BoW representation. *Document* could be just a word, a sentence, a tweet, a paragraph or even a book, anything which is tokenized. `gensim.corpora.Dictionary.doc2bow` method get the document as input, simultaneously looks at the dictionary and says us which words (in word ids) *there are* & how many *repetition* they have on this document, in tuples. For example, a BoW representation of a document like `[(144, 2), (8, 1)]` tells us the word associated with id = 144 repeated 2 times in this document and word id = 8 repeated only once (in **this** document).\n",
        "\n",
        "> **Tfidf representation**: TF-IDF model would be simply obtained by training. Training process would be done by looking at words in dictionary and calculate the inverse frequencies of them in the main corpus. Then Tfidf representation gives us a statistical measure that evaluates *how relevant a word is to a document in a corpus*. This is done by multplying two factors: *inverse word frequency* of the word in the whole corpus, and *how many times this word appeared in this document*. For example, suppose for the same above document, `[(144, 0.378), (8, 0.129)]` is the Tfidf representation which tells us word ids = 144 and 8 *are* in the document and their *tf-idf statistical measures* are 0.378 and 0.129 respectively. For more information you can see [tf-idf wikipedia](https://en.wikipedia.org/wiki/Tfâ€“idf)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Now that we've reached here, it will be useful to train another model. **Latent Semantic Indexing** or **LSI** model first published in [Deerwester et al. (1990): Indexint by Latent Semantic Analysis](https://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf). I just know this analysis works on *singular value decomposition* method and I want to know about the model more!\n",
        "\n",
        "Let's just initiate the `gensim.models.LsiModel` object with the training corpus in Tfidf representation for train a LSI model. Some other parameters needed like the dictionary as `id2word` and `num_topics` that I just guessed to 10."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC1O4vBB4jsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c384b8-4f18-42f1-ffb3-86a210f244e4"
      },
      "source": [
        "# train a LSI model with training corpus in tfidf\n",
        "lsi_model = models.LsiModel(train_corpus_tfidf, id2word=dictionary, num_topics=10)\n",
        "# tfidf->fold-in-lsi\n",
        "train_corpus_lsi = lsi_model[train_corpus_tfidf]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 05:11:52,695 : INFO : using serial LSI version on this node\n",
            "2021-04-04 05:11:52,705 : INFO : updating model with new documents\n",
            "2021-04-04 05:11:52,733 : INFO : preparing a new chunk of documents\n",
            "2021-04-04 05:11:52,747 : INFO : using 100 extra samples and 2 power iterations\n",
            "2021-04-04 05:11:52,751 : INFO : 1st phase: constructing (1613, 110) action matrix\n",
            "2021-04-04 05:11:52,768 : INFO : orthonormalizing (1613, 110) action matrix\n",
            "2021-04-04 05:11:52,856 : INFO : 2nd phase: running dense svd on (110, 104) matrix\n",
            "2021-04-04 05:11:52,868 : INFO : computing the final decomposition\n",
            "2021-04-04 05:11:52,870 : INFO : keeping 10 factors (discarding 80.171% of energy spectrum)\n",
            "2021-04-04 05:11:52,875 : INFO : processed documents up to #104\n",
            "2021-04-04 05:11:52,879 : INFO : topic #0(1.774): -0.401*\"news\" + -0.238*\"#lfc\" + -0.190*\"breaking\" + -0.173*\"will\" + -0.150*\"has\" + -0.145*\"be\" + -0.138*\"@lfc\" + -0.126*\"team\" + -0.125*\"out\" + -0.115*\"he\"\n",
            "2021-04-04 05:11:52,882 : INFO : topic #1(1.602): -0.579*\"news\" + -0.259*\"breaking\" + 0.162*\"has\" + -0.158*\"10pm\" + -0.153*\"big\" + -0.147*\"11pm\" + 0.136*\"out\" + -0.121*\"@arsenal\" + 0.110*\"team\" + -0.108*\"positive\"\n",
            "2021-04-04 05:11:52,883 : INFO : topic #2(1.470): 0.280*\"out\" + 0.242*\"#nufc\" + 0.232*\"#afc\" + 0.217*\"#thfc\" + -0.216*\"money\" + 0.198*\"ðŸ˜”\" + 0.197*\"has\" + -0.195*\"some\" + -0.193*\"follow\" + -0.169*\"#ad\"\n",
            "2021-04-04 05:11:52,885 : INFO : topic #3(1.438): 0.391*\"#lfc\" + -0.231*\"#nufc\" + -0.224*\"ðŸ˜”\" + -0.200*\"#afc\" + -0.186*\"out\" + -0.168*\"money\" + -0.166*\"follow\" + 0.154*\"team\" + -0.148*\"some\" + -0.144*\"#ad\"\n",
            "2021-04-04 05:11:52,890 : INFO : topic #4(1.395): 0.563*\"ðŸ˜”\" + 0.444*\"#nufc\" + 0.241*\"#lfc\" + -0.204*\"#afc\" + 0.161*\"season\" + -0.155*\"#thfc\" + -0.138*\"has\" + -0.122*\"kane\" + -0.122*\"harry\" + -0.120*\"aubameyang\"\n",
            "2021-04-04 05:11:52,892 : INFO : LsiModel lifecycle event {'msg': 'trained LsiModel(num_terms=1613, num_topics=10, decay=1.0, chunksize=20000) in 0.19s', 'datetime': '2021-04-04T05:11:52.892220', 'gensim': '4.0.1', 'python': '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ]
    },
    {
      "source": [
        "Note that we *piplined* models as *bow->tfidf->fold-in-lsi* and wrapped them over the training corpus."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Before representing documents in fold-in-lsi representation, we print 10 topics that we ordered. I've read the topics obtained by the model and believe me, it reminded me to black days of Liverpool FC in this season :(\n",
        "\n",
        "Read these topics with *hope in your heart*:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C5t7o3LBGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd1c616-f9aa-4f74-b15f-6375ccde5066"
      },
      "source": [
        "lsi_model.print_topics()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-04-04 05:12:28,226 : INFO : topic #0(1.774): -0.401*\"news\" + -0.238*\"#lfc\" + -0.190*\"breaking\" + -0.173*\"will\" + -0.150*\"has\" + -0.145*\"be\" + -0.138*\"@lfc\" + -0.126*\"team\" + -0.125*\"out\" + -0.115*\"he\"\n",
            "2021-04-04 05:12:28,238 : INFO : topic #1(1.602): -0.579*\"news\" + -0.259*\"breaking\" + 0.162*\"has\" + -0.158*\"10pm\" + -0.153*\"big\" + -0.147*\"11pm\" + 0.136*\"out\" + -0.121*\"@arsenal\" + 0.110*\"team\" + -0.108*\"positive\"\n",
            "2021-04-04 05:12:28,271 : INFO : topic #2(1.470): 0.280*\"out\" + 0.242*\"#nufc\" + 0.232*\"#afc\" + 0.217*\"#thfc\" + -0.216*\"money\" + 0.198*\"ðŸ˜”\" + 0.197*\"has\" + -0.195*\"some\" + -0.193*\"follow\" + -0.169*\"#ad\"\n",
            "2021-04-04 05:12:28,287 : INFO : topic #3(1.438): 0.391*\"#lfc\" + -0.231*\"#nufc\" + -0.224*\"ðŸ˜”\" + -0.200*\"#afc\" + -0.186*\"out\" + -0.168*\"money\" + -0.166*\"follow\" + 0.154*\"team\" + -0.148*\"some\" + -0.144*\"#ad\"\n",
            "2021-04-04 05:12:28,290 : INFO : topic #4(1.395): 0.563*\"ðŸ˜”\" + 0.444*\"#nufc\" + 0.241*\"#lfc\" + -0.204*\"#afc\" + 0.161*\"season\" + -0.155*\"#thfc\" + -0.138*\"has\" + -0.122*\"kane\" + -0.122*\"harry\" + -0.120*\"aubameyang\"\n",
            "2021-04-04 05:12:28,299 : INFO : topic #5(1.362): 0.305*\"#lfc\" + 0.256*\"#afc\" + 0.208*\"aubameyang\" + -0.192*\"ðŸ˜”\" + 0.190*\"same\" + 0.169*\"team\" + -0.161*\"will\" + -0.126*\"manchester\" + 0.123*\"furious\" + 0.119*\"some\"\n",
            "2021-04-04 05:12:28,305 : INFO : topic #6(1.317): 0.300*\"has\" + 0.232*\"left\" + 0.210*\"jurgen\" + 0.207*\"been\" + 0.168*\"sacked\" + 0.164*\"klopp\" + 0.163*\"@bcfc\" + -0.133*\"#lfc\" + -0.128*\"all\" + 0.127*\"@lfc\"\n",
            "2021-04-04 05:12:28,308 : INFO : topic #7(1.294): -0.380*\"ðŸ˜”\" + 0.221*\"out\" + 0.212*\"lamptey\" + 0.212*\"tariq\" + 0.199*\"season\" + 0.194*\"@david_ornstein\" + 0.183*\"according\" + -0.179*\"all\" + -0.115*\"enough\" + -0.111*\"#omi\"\n",
            "2021-04-04 05:12:28,312 : INFO : topic #8(1.288): 0.248*\"aubameyang\" + -0.241*\"out\" + -0.237*\"has\" + 0.235*\"#afc\" + -0.234*\"left\" + 0.213*\"ðŸ‘€\" + 0.191*\"klopp\" + 0.167*\"jurgen\" + 0.166*\"german\" + 0.159*\"dropped\"\n",
            "2021-04-04 05:12:28,315 : INFO : topic #9(1.271): 0.258*\"we\" + -0.202*\"manchester\" + 0.194*\"donâ€™t\" + 0.177*\"it\" + -0.139*\"liverpool\" + -0.133*\"gunnar\" + -0.133*\"ole\" + -0.124*\"city\" + -0.123*\"united\" + 0.121*\"think\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '-0.401*\"news\" + -0.238*\"#lfc\" + -0.190*\"breaking\" + -0.173*\"will\" + -0.150*\"has\" + -0.145*\"be\" + -0.138*\"@lfc\" + -0.126*\"team\" + -0.125*\"out\" + -0.115*\"he\"'),\n",
              " (1,\n",
              "  '-0.579*\"news\" + -0.259*\"breaking\" + 0.162*\"has\" + -0.158*\"10pm\" + -0.153*\"big\" + -0.147*\"11pm\" + 0.136*\"out\" + -0.121*\"@arsenal\" + 0.110*\"team\" + -0.108*\"positive\"'),\n",
              " (2,\n",
              "  '0.280*\"out\" + 0.242*\"#nufc\" + 0.232*\"#afc\" + 0.217*\"#thfc\" + -0.216*\"money\" + 0.198*\"ðŸ˜”\" + 0.197*\"has\" + -0.195*\"some\" + -0.193*\"follow\" + -0.169*\"#ad\"'),\n",
              " (3,\n",
              "  '0.391*\"#lfc\" + -0.231*\"#nufc\" + -0.224*\"ðŸ˜”\" + -0.200*\"#afc\" + -0.186*\"out\" + -0.168*\"money\" + -0.166*\"follow\" + 0.154*\"team\" + -0.148*\"some\" + -0.144*\"#ad\"'),\n",
              " (4,\n",
              "  '0.563*\"ðŸ˜”\" + 0.444*\"#nufc\" + 0.241*\"#lfc\" + -0.204*\"#afc\" + 0.161*\"season\" + -0.155*\"#thfc\" + -0.138*\"has\" + -0.122*\"kane\" + -0.122*\"harry\" + -0.120*\"aubameyang\"'),\n",
              " (5,\n",
              "  '0.305*\"#lfc\" + 0.256*\"#afc\" + 0.208*\"aubameyang\" + -0.192*\"ðŸ˜”\" + 0.190*\"same\" + 0.169*\"team\" + -0.161*\"will\" + -0.126*\"manchester\" + 0.123*\"furious\" + 0.119*\"some\"'),\n",
              " (6,\n",
              "  '0.300*\"has\" + 0.232*\"left\" + 0.210*\"jurgen\" + 0.207*\"been\" + 0.168*\"sacked\" + 0.164*\"klopp\" + 0.163*\"@bcfc\" + -0.133*\"#lfc\" + -0.128*\"all\" + 0.127*\"@lfc\"'),\n",
              " (7,\n",
              "  '-0.380*\"ðŸ˜”\" + 0.221*\"out\" + 0.212*\"lamptey\" + 0.212*\"tariq\" + 0.199*\"season\" + 0.194*\"@david_ornstein\" + 0.183*\"according\" + -0.179*\"all\" + -0.115*\"enough\" + -0.111*\"#omi\"'),\n",
              " (8,\n",
              "  '0.248*\"aubameyang\" + -0.241*\"out\" + -0.237*\"has\" + 0.235*\"#afc\" + -0.234*\"left\" + 0.213*\"ðŸ‘€\" + 0.191*\"klopp\" + 0.167*\"jurgen\" + 0.166*\"german\" + 0.159*\"dropped\"'),\n",
              " (9,\n",
              "  '0.258*\"we\" + -0.202*\"manchester\" + 0.194*\"donâ€™t\" + 0.177*\"it\" + -0.139*\"liverpool\" + -0.133*\"gunnar\" + -0.133*\"ole\" + -0.124*\"city\" + -0.123*\"united\" + 0.121*\"think\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "source": [
        "Then print first 5 tweets representation in fold-in-lsi. Note that we have 10 topics and the representation shows the relateness of each tweets to each one of 10 topics."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e59_on5K-1eJ",
        "outputId": "73512960-3608-4e40-dc6a-02166b5382be"
      },
      "source": [
        "for count, (tweet_lsi, tweet_text) in enumerate(zip(train_corpus_lsi, MyTexts(10))):\n",
        "    print(tweet_lsi, tweet_text, '\\n---')\n",
        "    if count == 5: break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, -0.04861963696522687), (1, 0.02614180540190273), (2, -0.013537251848193124), (3, 0.0016044318514834319), (4, -0.015416615212562145), (5, -0.010980581726811846), (6, 0.05679758723532831), (7, -0.007275692716576178), (8, 0.012138058118968094), (9, -0.010483042063101715)] ['statement', 'chelsea', 'fc', 'is', 'disgusted', 'with', 'posts', 'on', 'social', 'media', 'this', 'evening', 'targeting', 'west', 'bromwich', 'albion', 'player', 'callum', 'robinson', '#cfc'] \n---\n[(0, -0.04743868296514272), (1, -0.011337340781317722), (2, 0.10406379211482553), (3, -0.08082195369033182), (4, -0.09977622606145477), (5, 0.13535018506832436), (6, -0.062217162449585695), (7, -0.015246985119146397), (8, 0.17476093308953192), (9, -0.008777367833210809)] ['david', 'luiz', 'could', 'need', 'surgery', '#afc'] \n---\n[(0, -0.17661357974789188), (1, 0.07747152742326434), (2, -0.016897596975931274), (3, 0.1562200482516766), (4, 0.04223098125836869), (5, 0.0006513178763686359), (6, -0.13132871819521363), (7, -0.08954834316097088), (8, -0.22467101678412604), (9, -0.17457097247466385)] ['confirmed', 'liverpool', 'will', 'meet', 'with', 'erling', \"haaland's\", 'representatives', 'on', 'friday', '#lfc'] \n---\n[(0, -0.11462649672077915), (1, 0.014592444115943673), (2, -0.029170329685687287), (3, -0.03516800771325117), (4, -0.07482317324894426), (5, -0.11194640350014368), (6, -0.1154643861459454), (7, -0.1754808464002354), (8, -0.014739320838251945), (9, 0.10979646538227113)] ['the', 'mind', 'blowing', 'licence', 'to', 'be', 'announced', 'by', '@ecomi_', 'well', 'played', '@vevecollectible', 'the', 'company', 'in', 'great', 'hands', '#omi'] \n---\n[(0, -0.16735730545873773), (1, 0.1158119463636804), (2, 0.16927347838582285), (3, 0.02531187685121864), (4, 0.05355467785245896), (5, 0.102515498482607), (6, -0.04982623363799239), (7, 0.10394193030055968), (8, -0.18274263717439126), (9, -0.020672534678152506)] ['sergio', 'ramos', 'set', 'to', 'be', 'out', 'for', 'a', 'month', 'due', 'to', 'injury', '#lfc'] \n---\n[(0, -0.15931987027077713), (1, 0.12374107996485756), (2, 0.06186591538342217), (3, -0.043956962245251315), (4, 0.013989085890163313), (5, -0.09022984779278999), (6, -0.0559757832215728), (7, 0.08039329897032325), (8, -0.21382077739506672), (9, 0.03195669894721369)] ['bayern', 'munich', 'have', 'confirmed', 'robert', 'lewandowski', 'will', 'be', 'out', 'for', '4', 'weeks', 'with', 'a', 'knee', 'injury.'] \n---\n"
          ]
        }
      ]
    }
  ]
}