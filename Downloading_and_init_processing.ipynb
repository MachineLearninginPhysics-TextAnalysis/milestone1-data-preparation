{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Downloading and some initial processing ######################\n",
    "# (Ù”Note: more advanced processing is done in another file)\n",
    "\n",
    "\"\"\"\n",
    "The code has been made by a group of Machine Learning in Physics course, in 2021.\n",
    "Member of the group: hamid Pour Mohammad, Saeid Entezari, Mahdi Naghi Lou (and Reza Bakhoda)\n",
    "Extraction of data happens in 3 cells here; first, second and third cells.\n",
    "Please note that you may see some errors when you are collecting the data from twitter--because of VPN limits,\n",
    "twitter database, internet speed, etc. Thus, you may need to collect them by part.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tweepy\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from numpy import array, add, zeros, asmatrix, savetxt\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import Counter\n",
    "\n",
    "# input your credentials here: please note that everyone has special credentials, for the special purpose.\n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "\n",
    "# building a portal:\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "alp=('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "     'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ') # english alphabet, and the space\n",
    "\n",
    "def true_text(text): # making sure that each character is in 'alp'\n",
    "    rebuild=''\n",
    "    for t in text:\n",
    "        if t in alp: rebuild+=t\n",
    "    return(rebuild)\n",
    "\n",
    "def connector(df): # connecting the items of the dataframe\n",
    "    connected=[]\n",
    "    for i in df:\n",
    "        connected = connected + i\n",
    "    return(connected)\n",
    "   \n",
    "ps = PorterStemmer() # to make each word in a 'stem' condition; it helps us to find the root of each word\n",
    "\n",
    "most_words_list=[] # most words used in all classes\n",
    "\n",
    "nt=4000 # number of tweets that you want to be collected for each tagword.\n",
    "\n",
    "# reading the file of tags (external csv file):\n",
    "tags=pd.read_csv('tags.csv')\n",
    "tag_list=array(tags['the_tags'])\n",
    "\n",
    "# Making a frame, that will contain some lists. Each list is for one tag. We will put all these lists together.\n",
    "frames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN love...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lenght</th>\n",
       "      <th>tag</th>\n",
       "      <th>built_in_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1371947094085402628</td>\n",
       "      <td>AiCuddles</td>\n",
       "      <td>2021-03-16 22:10:51</td>\n",
       "      <td>['hi', 'what', 'did', 'i', 'do', 'wrong', 'lov...</td>\n",
       "      <td>10</td>\n",
       "      <td>love</td>\n",
       "      <td>#love-filter:retweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1371946904012140545</td>\n",
       "      <td>todaygame</td>\n",
       "      <td>2021-03-16 22:10:06</td>\n",
       "      <td>['medicalintuit', 'healer', 'interview', 'auth...</td>\n",
       "      <td>9</td>\n",
       "      <td>love</td>\n",
       "      <td>#love-filter:retweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1371946895174737924</td>\n",
       "      <td>CSCarrigan</td>\n",
       "      <td>2021-03-16 22:10:04</td>\n",
       "      <td>['medicalintuit', 'healer', 'interview', 'auth...</td>\n",
       "      <td>9</td>\n",
       "      <td>love</td>\n",
       "      <td>#love-filter:retweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1371946895157919748</td>\n",
       "      <td>UkHealing</td>\n",
       "      <td>2021-03-16 22:10:04</td>\n",
       "      <td>['medicalintuit', 'healer', 'interview', 'auth...</td>\n",
       "      <td>9</td>\n",
       "      <td>love</td>\n",
       "      <td>#love-filter:retweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1371946852464091136</td>\n",
       "      <td>JanderlynWMusic</td>\n",
       "      <td>2021-03-16 22:09:53</td>\n",
       "      <td>['thank', 'you', 'lord', 'for', 'all', 'you', ...</td>\n",
       "      <td>17</td>\n",
       "      <td>love</td>\n",
       "      <td>#love-filter:retweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user           created_at  \\\n",
       "0  1371947094085402628        AiCuddles  2021-03-16 22:10:51   \n",
       "1  1371946904012140545        todaygame  2021-03-16 22:10:06   \n",
       "2  1371946895174737924       CSCarrigan  2021-03-16 22:10:04   \n",
       "3  1371946895157919748        UkHealing  2021-03-16 22:10:04   \n",
       "4  1371946852464091136  JanderlynWMusic  2021-03-16 22:09:53   \n",
       "\n",
       "                                                text  text_lenght   tag  \\\n",
       "0  ['hi', 'what', 'did', 'i', 'do', 'wrong', 'lov...           10  love   \n",
       "1  ['medicalintuit', 'healer', 'interview', 'auth...            9  love   \n",
       "2  ['medicalintuit', 'healer', 'interview', 'auth...            9  love   \n",
       "3  ['medicalintuit', 'healer', 'interview', 'auth...            9  love   \n",
       "4  ['thank', 'you', 'lord', 'for', 'all', 'you', ...           17  love   \n",
       "\n",
       "            built_in_tag  \n",
       "0  #love-filter:retweets  \n",
       "1  #love-filter:retweets  \n",
       "2  #love-filter:retweets  \n",
       "3  #love-filter:retweets  \n",
       "4  #love-filter:retweets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the first iteration: (it takes about two minutes)\n",
    "# We want to build a 'tagword', and its related 'tagword-filter:retweets';\n",
    "# because a lot of tweets are repetitious.\n",
    "word='#'+tag_list[0] # here is just the 'tagword'\n",
    "new_search=word\n",
    "new_search+=\"-filter:retweets\" # here is the 'tagword-filter:retweets'\n",
    "print(f'IN {word}...') # it shows in which tagword we are.\n",
    "\n",
    "# Collecting the data for the first tagword, with a generator:\n",
    "tweets=tweepy.Cursor(api.search, q=new_search, count=100, \n",
    "                           lang=\"en\",\n",
    "                           since=\"2021-03-01\").items(nt)\n",
    "\n",
    "# Separating the features of each tweet:\n",
    "tweet_information = [[tweet.id, tweet.user.screen_name, tweet.created_at, tweet.text] \n",
    "                     for tweet in tweets]\n",
    "\n",
    "# Creating a Pandas Dataframe from 'tweet_information' data:\n",
    "tweet_information = pd.DataFrame(data=tweet_information, \n",
    "                    columns=['id', 'user', \"created_at\", 'text'])\n",
    "\n",
    "# cleaning the tweets:\n",
    "tweet_information['text']=tweet_information['text'].str.lower() # convert text to lowercase\n",
    "tweet_information['text']=tweet_information['text'].replace('#', ' ').replace('\\n', ' ') # remove # and \\n\n",
    "tweet_information['text']=tweet_information['text'].apply(lambda text: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split()))\n",
    "tweet_information['text']=tweet_information['text'].replace('_', ' ')  # eliminate the _ sign\n",
    "tweet_information['text']=tweet_information['text'].apply(lambda text: true_text(text)) # just to make sure that the tweet is a text\n",
    "tweet_information['text']=tweet_information['text'].apply(lambda text: text.split()) # splitting the text\n",
    "tweet_information['text']=tweet_information['text'].apply(lambda text: list(ps.stem(word) for word in text)) # to use the base form of the words\n",
    "\n",
    "most_words = tweet_information['text'] # most used words for this tagword\n",
    "most_words = connector(most_words) # connecting the items of this dataframe\n",
    "most_words = Counter(\" \".join(most_words).split()).most_common(int(nt/15))\n",
    "most_words = pd.DataFrame(most_words, columns=['Word', 'Frequency'])\n",
    "most_words_list = most_words_list + list(most_words['Word']) # add to most used words list\n",
    "\n",
    "tweet_information['text_lenght']= tweet_information['text'].apply(lambda text: len(text))\n",
    "tweet_information['tag']=true_text(word) # writing the related 'tagword'\n",
    "tweet_information['built_in_tag']=new_search # writing the related 'tagword-filter:retweets'\n",
    " \n",
    "frames.append(tweet_information) # appending this tweet_information to 'frames' list\n",
    "\n",
    "tweet_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN covid...\n",
      "IN fashion...\n",
      "IN business...\n",
      "IN travel...\n",
      "IN technology...\n",
      "IN science...\n",
      "IN political...\n",
      "IN physics...\n",
      "IN pharmacy...\n",
      "IN pet...\n",
      "IN medical...\n",
      "IN marketing...\n",
      "IN solar...\n",
      "IN job...\n",
      "IN innovation...\n",
      "IN gym...\n",
      "IN government...\n",
      "IN food...\n",
      "IN engineering...\n",
      "IN election...\n",
      "IN culture...\n",
      "IN bodybuilding...\n",
      "IN biology...\n",
      "IN art...\n",
      "IN architecture...\n",
      "IN animal...\n",
      "Downloading finished.\n",
      "Some initial processing has been finished.\n"
     ]
    }
   ],
   "source": [
    "# NOW, let's do it for all 'tagword's:\n",
    "\n",
    "# (Note: it takes about 2 hours)\n",
    "\n",
    "# We did this iteration for the first tagword, so we don't need to do it again; just others:\n",
    "tag_list=tag_list[1:] \n",
    "\n",
    "for i in tag_list:\n",
    "    word='#'+i\n",
    "    new_search=word\n",
    "    new_search+=\"-filter:retweets\"\n",
    "    print(f'IN {word}...')\n",
    "\n",
    "    tweets=tweepy.Cursor(api.search, q=new_search, count=100, \n",
    "                               lang=\"en\",\n",
    "                               since=\"2021-03-01\").items(nt)\n",
    "    \n",
    "    tweet_information = [[tweet.id, tweet.user.screen_name, tweet.created_at, tweet.text] \n",
    "                         for tweet in tweets]\n",
    "\n",
    "    tweet_information = pd.DataFrame(data=tweet_information, \n",
    "                        columns=['id', 'user', \"created_at\", 'text'])\n",
    "\n",
    "    tweet_information['text']=tweet_information['text'].str.lower()\n",
    "    tweet_information['text']=tweet_information['text'].replace('#', ' ').replace('\\n', ' ')    \n",
    "    tweet_information['text']=tweet_information['text'].apply(lambda text: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split()))\n",
    "    tweet_information['text']=tweet_information['text'].replace('_', ' ')  \n",
    "    tweet_information['text']=tweet_information['text'].apply(lambda text: true_text(text))\n",
    "    tweet_information['text']=tweet_information['text'].apply(lambda text: text.split())\n",
    "    tweet_information['text']=tweet_information['text'].apply(lambda text: list(ps.stem(word) for word in text))\n",
    "    \n",
    "    most_words = tweet_information['text']\n",
    "    most_words = connector(most_words)\n",
    "    most_words = Counter(\" \".join(most_words).split()).most_common(int(nt/15))\n",
    "    most_words = pd.DataFrame(most_words, columns=['Word', 'Frequency'])\n",
    "    most_words_list = most_words_list + list(most_words['Word'])\n",
    "    \n",
    "    tweet_information['text_lenght']= tweet_information['text'].apply(lambda text: len(text))\n",
    "    tweet_information['tag']=true_text(word)\n",
    "    tweet_information['built_in_tag']=new_search\n",
    "\n",
    "    frames.append(tweet_information)\n",
    "    \n",
    "most_words_list = Counter(\" \".join(most_words_list).split()).most_common(10*nt)\n",
    "most_words_list = pd.DataFrame(most_words_list, columns=['Word', 'Frequency'])\n",
    "most_words_list_result=most_words_list.reset_index(drop=True)\n",
    "most_words_list_result.to_csv('most_words_list_result.csv',index=False)\n",
    "\n",
    "most_words_list = list(most_words_list['Word'])\n",
    "\n",
    "# concatinating all 'tweet_information's together:\n",
    "result = pd.concat(frames)\n",
    "\n",
    "# resetting the index, and saving:\n",
    "result=result.reset_index(drop=True)\n",
    "result.to_csv('tweet_information.csv',index=False)\n",
    "\n",
    "print('Downloading finished.')\n",
    "print('Some initial processing has been finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
